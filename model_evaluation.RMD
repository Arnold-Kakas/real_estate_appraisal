---
title: "Real estate advertisements data EDA"
author: "Arnold Kakaš"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Advertisement data EDA Price prediction model

### Introduction
<br>
In this post I will go through <strong> Exploratory Data Analyses (EDA) </strong> of real estate advertisements data and evaluation of <strong> price prediction model </strong> based on this dataset.

<br>

Data is scraped from [Nehnutelnosti](https://www.nehnutelnosti.sk/) website that specializes in real estate listings and services. I will go through the scraping process and initial data cleaning in more details in the next blog post.


### Data cleaning
<br>
Loading of libraries. For this part I like to use [packman](https://cran.r-project.org/web/packages/pacman/index.html) package
```{r message=FALSE, warning=FALSE}
# load libs
pacman::p_load(rio,
               gridExtra,
               tidyverse,
               tidymodels,
               corrplot,
               ggpubr,
               plotly,
               gt,
               DT,
               GGally,
               patchwork,
               vip,
               janitor,
               sf)#,
               #spdep,
               #rgeos,
               #rgdal,
               #sfdep)
```

<br>
Next step is to load the analysed data. Usually for imports I use [rio](https://cran.r-project.org/web/packages/rio/index.html) package. <br>
As you can see, data is stored in RDS format. I prefer this format due to several reasons: 
* size of the file is lower compared to csv or xlsx, which are usually the ones that we as data analysts are provided with.
* RDS format preserves the entire R object structure, including data frames, matrices, lists, functions, and metadata.Therefore you can save objects like dataframes with factors, trained models, etc. 
* RDS is optimized for reading and writing by the R. Therefore importing or exporting data is very quick.
<br>
Before the EDA, I need to clean up the scraped data.
<br>
I import two datasets. First one is advertisements.RDS file which contains links and basic information from the initial [webpage](https://www.nehnutelnosti.sk/byty/predaj/). I split one of the variables to obtain type of real estate.
This dataframe is joined to second one, advertisements_complete_geocoded.RDS, that contains more detailed information of each of advertisements (these informations are visible after opening the links in first dataframe) and is already geocoded with [tidygeocoder](https://jessecambon.github.io/tidygeocoder/) package using OSM Nominatim API.
<br>
Afterwards follows the <strong> data cleaning </strong> and <strong> data wrangling </strong> using different methods:

* replacing
* filtering
* imputing missing values
* converting to correct data type/creating factors
* removing redundant variables (based on further data analysis, e.g. due to high percentage of missing values or no correlation with price)
* renaming of variables
* grouping
* using geospatial joins
* removing outliers
<br>
This process is part of <strong> feature engineering </strong> for subsequent prediction model creation.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# load data for EDA
advertisements <- import("data/advertisements.RDS")
advertisements <- advertisements %>% separate(type_of_real_estate, c("type", "area"), sep = " • ", remove = TRUE) %>% select(link, type)


scraped_cleaned <- import("data/advertisements_complete_geocoded.RDS") %>% 
  left_join(advertisements, by = "link", multiple = "first", keep = FALSE) %>% 
  clean_names() %>%
  mutate(pocet_izieb_miestnosti = as.numeric(pocet_izieb_miestnosti),
         uzit_plocha = str_replace(str_replace(uzit_plocha, ",", "."), " m2", ""),
         energie = str_replace(str_replace(energie, ",", "."), " €/mesiac", ""),
         provizia_zahrnuta_v_cene = str_replace_na(provizia_zahrnuta_v_cene, "Nie"),
         rooms = case_when(type == "1 izbový byt" ~ 1,
                           type == "2 izbový byt" ~ 2,
                           type == "3 izbový byt" ~ 3,
                           type == "4 izbový byt" ~ 4,
                           type == "5 a viac izbový byt" ~ 5,
                           type == "Garsónka" ~ 1,
                           type == "Dvojgarsónka" ~ 2, 
                           .default = NA),
         pocet_izieb_miestnosti = coalesce(pocet_izieb_miestnosti, rooms, pocet_izieb_miestnosti)) %>% 
  mutate_at(c('index_of_living',
              'uzit_plocha',
              'energie',
              'pocet_nadzemnych_podlazi', 
              'podlazie', 
              'pocet_izieb_miestnosti', 
              'rok_vystavby', 
              'rok_poslednej_rekonstrukcie', 
              'pocet_balkonov', 
              'pocet_lodzii'), as.numeric) %>% 
  select(-link, -info_text, -address) %>% 
  filter(pocet_izieb_miestnosti < 10 & !is.na(pocet_izieb_miestnosti)) %>% 
  mutate(
         type = coalesce(type, case_when(pocet_izieb_miestnosti == 1 ~ "1 izbový byt",
                           pocet_izieb_miestnosti == 2 ~ "2 izbový byt",
                           pocet_izieb_miestnosti == 3 ~ "3 izbový byt",
                           pocet_izieb_miestnosti == 4 ~ "4 izbový byt",
                           pocet_izieb_miestnosti >= 5 ~ "5 a viac izbový byt"))) %>% 
  select(-rooms) %>% 
  filter(!(type %in% c("Apartmán", "Mezonet", "Iný byt", "Loft"))) %>% 
  mutate(rooms = case_when(type == "1 izbový byt" ~ 1,
                           type == "2 izbový byt" ~ 2,
                           type == "3 izbový byt" ~ 3,
                           type == "4 izbový byt" ~ 4,
                           type == "5 a viac izbový byt" ~ 5,
                           type == "Garsónka" ~ 1,
                           type == "Dvojgarsónka" ~ 2, 
                           .default = NA)) %>% 
  select(-c(rok_vystavby,
            rok_poslednej_rekonstrukcie,
            typ_konstrukcie,
            pocet_balkonov,
            pocet_lodzii,
            orientacia,
            energie,
            pivnica,
            vytah,
            pocet_izieb_miestnosti,
            pocet_nadzemnych_podlazi,
            podlazie)) %>% 
  dplyr::rename(index = index_of_living) %>% 
  dplyr::rename(condition = stav) %>% 
  dplyr::rename(area = uzit_plocha) %>% 
  dplyr::rename(provision = provizia_zahrnuta_v_cene) %>%
  dplyr::rename(certificate = energeticky_certifikat)

scraped_cleaned_wo_geometry <- scraped_cleaned
st_geometry(scraped_cleaned_wo_geometry) <- NULL

number_of_ads <- scraped_cleaned_wo_geometry %>% group_by(name_nsi) %>% tally() %>% arrange(n)

index_district <- scraped_cleaned_wo_geometry %>% 
  filter(!is.na(price)) %>%
  group_by(name_nsi) %>% 
  mutate(index_mean_name_nsi = mean(index, na.rm = TRUE)) %>%
  ungroup() %>%
  select(name_nsi, district, index_mean_name_nsi) %>% 
  distinct() %>% 
  mutate(index_mean_name_nsi = if_else(index_mean_name_nsi > 0, index_mean_name_nsi, NA)) %>% 
  group_by(district) %>% 
  summarize(index_mean_district = mean(index_mean_name_nsi, na.rm = TRUE))

scraped_cleaned <- scraped_cleaned %>% 
  left_join(number_of_ads, by = "name_nsi", keep = FALSE) %>% 
  filter(n > 4) %>% 
  left_join(index_district, by = "district", multiple = "first", keep = FALSE) %>% 
  st_centroid()

districts <- import("data/geospatial_data/districts.RDS")  

scraped_cleaned <- districts %>% 
  st_join(scraped_cleaned, join = st_contains) %>% 
  select(-municipality, -district) %>% 
  dplyr::rename(district = NAME) %>% 
  mutate(district = as_factor(district),
    condition = as_factor(condition),
    type = as_factor(type),
    provision = as_factor(provision),
    certificate = as_factor(certificate),
    name_nsi = as_factor(name_nsi)
  ) %>% 
  select(-n, -address1, -address2) %>% 
  filter(!is.na(price))


scraped_cleaned_no_outliers <- scraped_cleaned %>% 
  group_by(name_nsi) %>% # group by municipality
  mutate(index = mean(as.numeric(index),na.rm = TRUE), # index of municipality
         index = if_else(index > 0, index, NA), # replace 0 index with NA
         IQR = IQR(price), 
         median_price = median(price),
         lower = median_price - 1.5*IQR, 
         upper = median_price + 1.5*IQR
         ) %>% 
  ungroup() %>% 
  filter(price >= lower & price <= upper) %>% # filter out outliers
  select(-lower, -upper, -median_price, -IQR)# remove variables

st_geometry(scraped_cleaned_no_outliers) <- NULL

# scraped_cleaned_no_outliers <- scraped_cleaned_no_outliers %>% 
#   group_by(district) %>% 
#   mutate(avg_price_district = mean(price, na.rm = TRUE),
#          )
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Remove all dataframes except of scraped_cleaned_no_outliers
rm(list = setdiff(ls(), "scraped_cleaned_no_outliers"))
gc()
```

### EDA {.tabset}

#### Overview
<br>
Let's check how the data look like
<br>
```{r}
head(scraped_cleaned_no_outliers) %>% 
  gt() %>% 
  tab_options(table.font.size = 9,
              table.width = 1100,
              table.layout = "auto")
```

<br>
I like to start the EDA with base summary function.
<br>
```{r message=FALSE, warning=FALSE}
 summary(scraped_cleaned_no_outliers) %>% 
  as.data.frame()  %>%
  select(-Var1) %>% 
  pivot_wider(names_from = Var2, values_from = Freq) %>% 
  unnest(everything()) %>% 
  dplyr::rename('district mean index' = 11) %>% 
  dplyr::rename(municipality = 2) %>% 
  gt() %>% 
  tab_options(table.font.size = 9,
              table.width = 1100,
              table.layout = "auto")
```
<br>
Most of the advertisements are from western part of Slovakia, we can see clear dominance of Bratislava city districts. Looking at the top 5 municipalities, only Banská Bystrica is located outside of the western part of country.
<br>
Price has quite big range from 1000 to almost 700k, but the middle 50 % lies within 70k range from 105,000 to 179,000. We will check the distribution later in histogram.
<br>
Index can have values from 0 to 10 and it is prepared by slovak startup [City Performer](https://cityperformer.com/). It is made up of six categories: environment, quality of housing, safety, transport, services and relaxation. That being said, it is not available for all places.
Middle 50 % are within 7.1 to 8.1.
<br>
There are 6 possible conditions + NA. The most common is complete reconstruction followed by partial reconstruction and new construction.
<br>
Area is one of the most important attributes impacting the price. As we can see, we it ranges from 1 squared meter to 215,000, both are obviously incorrect values. Similarily to price, we will look at distribution in histogram.
<br>
Provision tells us whether the provision of real estate agency is included. Mostly it is not (only part of the apartments are sold via real estate agency).
<br>
Information on energy certificate is mostly missing or the property does not have one. But in cases of high grade certificates we can expect positive impact on price.
<br>
Type and rooms are two variables showing very similar characteristic. Most common are apartments with 3 and 2 rooms. 
<br>
Mean index value of district is used as additional feature to evaluate the location quality for places where local index is not available. Still, there are places where even this could not be calculated, nor by grouping or spatial lag.

#### Price

Price is the variable I will try to predict.

```{r message=FALSE, warning=FALSE}
price1 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = price)) +
        geom_histogram(fill = "steelblue", binwidth = 10000) +
        scale_x_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  theme_minimal()
        
price2 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = log10(price))) +
        geom_histogram(fill = "steelblue") +
  theme_minimal()

grid.arrange(price1, price2, ncol = 1)
```
<br>
```{r}
price3 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(y = price)) +
  geom_boxplot(col = 'steelblue') + 
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  theme_minimal()

price4 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(y = log10(price))) +
  geom_boxplot(col = 'steelblue') + 
  theme_minimal()

grid.arrange(price3, price4, ncol = 2)
```

As we can see, the distribution is skewed to the right, log transformation help with this issue to some extent. 

#### District

```{r}
top_bottom_10_districts <- scraped_cleaned_no_outliers %>% 
  group_by(district) %>% 
  summarize(price = mean(price, na.rm = TRUE),
            adverts_count = n) %>% 
   filter(!between(dense_rank(price), 10 + 1, n() - 10)) %>% 
  arrange(desc(price))
```

```{r}
#n1 <- 
top_bottom_10_districts %>% 
  ggplot(aes(x = reorder(district, -price), y = price)) + 
  geom_bar(stat='summary', fun.y = "mean", fill='steelblue') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_y_continuous(breaks = seq(0, 350000, by = 50000), labels = comma) + 
  geom_text(aes(x = reorder(district, -price) ,y = price + 25000, label = adverts_count)) +
  geom_hline(yintercept = 135900, linetype="dashed", color = "red") #dashed line is median price 
# n2 <- ggplot(data=all, aes(x=Neighborhood)) + 
#         geom_histogram(stat='count')+ 
#         geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+ 
#         theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
# grid.arrange(n1, n2) 
```


#### Municipality

#### Index

```{r}
scraped_cleaned_no_outliers %>% 
  select(index) %>% 
  drop_na() %>% 
  ggplot(aes(x = index)) +
        geom_histogram(fill = "steelblue") +
  theme_minimal()
```


```{r message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  select(price, index) %>% 
  drop_na() %>% 
  ggplot(aes(x = index, y = price)) +
  geom_point(col = 'steelblue') + 
  geom_smooth(method = "lm", se = FALSE, color = "red", aes(group = 1)) +
  stat_regline_equation(label.x = 3, label.y = 650000) + # for regression equation 
  stat_cor(aes(method = "pearson", label = after_stat(rr.label)), label.x = 3, label.y = 600000) + # for R^2 
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma)
```
#### Condition

#### Area
<br>
```{r message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  filter(area < 200 & area > 10) %>% 
  ggplot(aes(x = area, y = price)) +
  geom_point(col = 'steelblue') + 
  geom_smooth(method = "lm", se = FALSE, color = "red", aes(group = 1)) +
  stat_regline_equation(label.x = 3, label.y = 650000) + # for regression equation 
  stat_cor(aes(method = "pearson", label = after_stat(rr.label)), label.x = 3, label.y = 600000) + # for R^2 
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma)
```
Area explains 24% of variance in price.

#### Provision

#### Certificate
<br>
```{r}
cert1 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = factor(certificate), y = price)) +
  geom_boxplot(col = 'steelblue') + 
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  theme_minimal() +
  labs(x = "type")

ggplotly(cert1)
```
#### Type
<br>
```{r}
type1 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = factor(type), y = price)) +
  geom_boxplot(col = 'steelblue') + 
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  theme_minimal() +
  labs(x = "type")

ggplotly(type1)
```


#### Mean District Index
```{r message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  select(price, index_mean_district) %>% 
  drop_na() %>% 
  ggplot(aes(x = index_mean_district, y = price)) +
  geom_point(col = 'steelblue') + 
  geom_smooth(method = "lm", se = FALSE, color = "red", aes(group = 1)) +
  stat_regline_equation(label.x = 3, label.y = 650000) + # for regression equation 
  stat_cor(aes(method = "pearson", label = after_stat(rr.label)), label.x = 3, label.y = 600000) + # for R^2 
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma)
```
#### Correlation 
<br>

```{r}
all_num_pred <- select_if(scraped_cleaned_no_outliers, is.numeric) %>% drop_na()

M <- cor(all_num_pred)

corrplot(M, method = 'number', order = 'AOE', type = 'upper', diag = FALSE)
```

There is a strong multicollinearity between index and mean district index. SPOILER ALERT Since the final model is an XGBoost model, I don't consider this a big issue. On the other hand, it may be beneficial in cases where there is no index value available (approx. 700 cases).
