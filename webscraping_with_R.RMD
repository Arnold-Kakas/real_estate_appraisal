---
title: "Web Scraping with R - Real estate data"
author: "Arnold Kakaš"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: flatly
    highlight: tango
    code_folding: hide
    toc: true
    toc_float: 
        collapsed: false
        smooth_scroll: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

## Caution
<br>
Before I get to the topic of this post, please note that this article is for <strong>informational purposes only</strong> and that any information contained below <strong>does not constitute legal advice</strong>. Therefore, before engaging in any scraping activities, you should get appropriate professional legal advice regarding your specific situation.

## Web scraping
<br>
In this blog post I will go through <strong>[web scraping](https://en.wikipedia.org/wiki/Web_scraping)</strong> using <strong>R programming language</strong>.
Before I dive into the process itself I would like to talk a bit about the topic in general.
<br>
Web scraping is the process of collecting content or (usually) structured data from websites in an automated manner (and usually in large amount). This definition naturally brings up the question of legality of such a process.
In general, web scraping is not illegal or prohibited per se (in EU, as of July 2023) . However, the use of data mining tools is legally risky due to several reasons:

* Intellectual property breach
* Contract breach
* Privacy concerns

To minimize concerns, scraping should be discreet, respect websites’ terms of service, check whether sites are using the robots.txt protocol to communicate that scraping is prohibited, avoid personal data scraping and, if it is necessary, make sure no GDPR violations are made and avoid scraping private or classified information ([Source](https://discoverdigitallaw.com/is-web-scraping-legal-short-guide-on-scraping-under-the-eu-jurisdiction/#IV_HOW_TO_SCRAPE_AND_NOT_BE_SUED)).
<br>
There are some general ethic principles that you should follow when you want to scrape data from web.
The most frequently mentioned are:

* If there is a public <strong>[API](https://en.wikipedia.org/wiki/API)</strong> that provides the data you are looking for, use it instead of scraping.
* Request data at a reasonable rate. So your scraping is not being harmful on the server and can't be confused with a <strong>[DDoS attack](https://en.wikipedia.org/wiki/Denial-of-service_attack)</strong>. 
* Respect other's intellectual property. Use the data to create new value from the data, not to duplicate and pass it as your own or illegally sell it.
* Do not scrape personal or private data or documents, respect the GDPR.
* View robots.txt file to check how the site should be crawled.
* Share what you can. If the data you scraped is in the public domain or you got permission to share it, then put it out there for other people to reuse it (e.g. on [GitHub](github.com) or [Kaggle](kaggle.com)). If you wrote a web scraper to access it, share its code so that others can benefit from it.
* Look for ways how to return value to website you are scraping, e.g. reference the website in an article or post, so you drive traffic to it.

You can find several articles dealing with this topic e.g. [This one on towardsdatascience.com](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01), or on [Data Fluency github page](https://monashdatafluency.github.io/python-web-scraping/section-5-legal-and-ethical-considerations/) or on [scrapingrobot page](https://scrapingrobot.com/blog/ethical-web-scraping/).
<br>
In this article I will scrape data from [Nehnutelnosti](https://www.nehnutelnosti.sk/) website that specializes in real estate listings and services. To follow ethical scraping principles, I made some preventive measures and actions: 

* This particular website uses robots.txt protocol. I am not scraping any part that is disallowed and in some parts of code I added _Sys.sleep()_ function to slow down the process so I request data at a reasonable rate.
* I scrape only public data I need and probably will limit this data even further after [EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis).
* I have publicly available [GitHub repo](https://github.com/Arnold-Kakas/real_estate_appraisal) where all code and data can be accessed.
<br>
<center>
![Nehnutelnosti website robots.txt protocol](data/Nehnutelnosti robots protocol.JPG)
</center>
<br>

## Scraping process {.tabset .tab-pills}
<br>
### Packages
<br>
As usual, I start with loading packages necessary for this project. I am using [packman](https://cran.r-project.org/web/packages/pacman/index.html) package and function _p_load()_ to do this. I has two advantages compared to traditional _library()_:

* if the library is not installed, it will do it automatically
* you can load multiple libraries at once

```{r cars, eval=FALSE, include=FALSE}
if (!require("pacman")) {
  install.packages("pacman")
}
pacman::p_load(
  rio,
  tidyverse,
  rvest, # scraping, part of tidyverse
  httr, # working with html
  RSelenium, # scraping in Google Chrome
  netstat, # free_port()
  doParallel, # parallel processing
  furrr # future map
)
```

There are three groups of packages in this project:

* for data loading and data wrangling - rio, tidyverse
* for web scraping - rvest, RSelenium, netstat, httr
* for parallel processing - doParallel and furrr

### Web elements
<br>
Regardless of what programming language or which package you choose for webscraping, you need to be able to find the elements in web page source code.
This can be easily done in you web browser (for this blog purposes I am using Google Chrome). You need to press *CRTL* + *SHIFT* + *I* to open DevTools. Now, when you hover over the code in the window, it will dynamically show you in browser which part of the page is it related to. Easier way how to get the correct element (or other web part) reference is to press *CRTL* + *SHIFT* + *C* and select the element directly on page.
<br>
<center>
![DevTools in Google Chrome](data/DevTools.JPG)
</center>
<br>

Once you found the correct element, you need to copy the CSS selector or XPath. Both can be used as arguments in rvest and RSelenium.
<br>
<center>
![Copying web element XPath](data/Web element Xpath-CSS selector.JPG)
</center>
<br>

Now you are ready to retrieve the content from page. Result, if you did everything correctly, can have different forms. It can be a single value, string, list etc.. Based on this you either specify more precisely what part of the content you need, or work with the result in R and use data wrangling functions to get the information you are looking for.

### Scraping part I. - rvest
<br>
One of the most common web scraping packages in R is [rvest](https://rvest.tidyverse.org/). It provides functions to access a public web page and query-specific elements using CSS selectors and XPath. This package does not run javascript, meaning that it retrieves the page html faster, but will miss any elements loaded with javascript after the initial page load.
Therefore this package is good option if you are scraping static pages.

In this example I start with creating a variable to store the source url: *https://www.nehnutelnosti.sk/slovensko/byty/predaj/?p[param1][from]=1000&p[param1][to]=&p[page]=*

Next I check the number of pages
<br>
<center>
![Number of pages with search results](data/Number of pages.jpg)
</center>
<br>

And lastly I prepare and run multisession to retrieve price, address, type of real estate and most important - link to the advertisement which will be used in next part with RSelenium package.
I also added *plan(sequential)* to stop the multisession, but I have to admit, I am not as familiar with parallel programming in R to fully understand the importance of this step.
<br>
```{r rvest_scraping}
# apartments page
site <- "https://www.nehnutelnosti.sk/slovensko/byty/predaj/?p[param1][from]=1000&p[param1][to]=&p[page]="

# scrape the number of pages
number_of_pages <- read_html(paste0(site, 1)) %>%
  html_nodes(xpath = '//*[@id="content"]/div[8]/div/div/div[1]/div[17]/div/div/ul/li[5]') %>%
  html_elements("a") %>%
  html_text(trim = TRUE) %>%
  as.numeric()

# create a cluster of worker processes (cores)
plan(multisession, workers = 6)

advertisements <- future_map_dfr(1:number_of_pages, function(i) {
  page_content <- read_html(paste0(site, i))
  
  price <- page_content %>%
    html_nodes(xpath = '//*[@class="advertisement-item--content__price col-auto pl-0 pl-md-3 pr-0 text-right mt-2 mt-md-0 align-self-end"]') %>%
    html_attr("data-adv-price")
  
  type_of_real_estate <- page_content %>%
    html_nodes(xpath = '//*[@class="advertisement-item--content__info"]') %>%
    html_text2()
  
  address <- page_content %>%
    html_nodes(xpath = '//*[@class="advertisement-item--content__info d-block text-truncate"]') %>%
    html_text2()
  
  link <- page_content %>%
    html_nodes(xpath = '//*[@class="mb-0 d-none d-md-block"]') %>%
    html_nodes("a") %>%
    html_attr("href")
  
  tibble(price = price, type_of_real_estate = type_of_real_estate, address = address, link = link)
})

plan(sequential)
```

### Scraping part II. - RSelenium
<br>
[RSelenium](https://cran.r-project.org/web/packages/RSelenium/index.html) Provides a set of R bindings for the *Selenium 2.0 WebDriver*. Unlike rvest, it runs a real web browser, so it will load any javascript contained in the webpage.
With this package you will be able to interact with web page e.g. scroll, click on button, fill in input forms etc..
On the other hand, use of this package requires specific set up, installed Java on your system and I faced multiple issues before I got it running properly. More on this topic will be covered in one of the future blogs.
<br>
I start with defining several helper functions. 
One is for unsuccessful navigation to page, this usually happened when I read too many pages and I had to delete browsing history. Sometimes this function created loop running several hours before I noticed (e.g. during night) but at least the code did crashed. I also tried creating function to delete the history in case of unsuccessful navigation, but it did not work and I am fine with this solution for the time being. 
Second one is for handling error in element search. In such cases this function returns NA.
Last one is similar to the previous, however in this case it returns NA in case when element is not found.
<br>
I also use `tryCatch()` function in scraping script to return NA in case of error.
<br>
```{r helper_functions}
# Define a function that handles the errors in page load
navigate_with_retry <- function(link, remDr) {
  success <- FALSE
  while (!success) {
    tryCatch(
      {
        remDr$navigate(link)
        Sys.sleep(5)
        success <- TRUE
      },
      error = function(e) {
        cat("Failed to navigate to", link, "- Retrying in 10 seconds...\n")
        clearCookies(remDr)
        Sys.sleep(10)
      }
    )
  }
}

# Define a wrapper function that handles the errors in element search
safe_find_element <- possibly(function(page, xpath) {
  page$findElement(using = "xpath", xpath)
}, NA)

# function to get text or return NA if element not found
get_text_or_na <- function(nodes) {
  tryCatch(
    {
      text <- nodes %>%
        html_text2() %>%
        as.character() # %>%
      # str_trim() %>%
      # str_squish()
      if (text == "") NA else text
    },
    error = function(e) {
      NA
    }
  )
}
```
<br>
Now I am splitting links to 10 batches. This does not has to be done since the code is working, but I keep it in case of some unexpected error occurrence during the scraping process in order to save at least part of the data. This may seem paranoid, but I learned to be causes the hard way. The next part of web scraping takes more than two days and you don't want to lose whole progress because of WiFi outage. 
I am also creating empty dataframe with all possible columns which will be fed later in the process.
<br>
```{r scrape_prep}
# Additional info from web

# number of splits
num_splits <- 10
split_size <- ceiling(nrow(advertisements) / num_splits)

# split the data frame into subsets
advertisments_list <- split(advertisements, rep(1:num_splits, each = split_size, length.out = nrow(advertisements)))

for (i in seq_along(advertisments_list)) {
  assign(paste0("advertisements_", i), advertisments_list[[i]])
}

# create empty dataframe outside of the loop to hold additional info
additional_info_df <- tibble(
  link = character(),
  info_text = character(),
  additional_characteristics = character(),
  index_of_living = character(),
  info_details = character(),
  stringsAsFactors = FALSE
)

```
<br>
Now I am getting to actual scraping with RSelenium. As already mentioned, to get it running I need to follow several steps. I am using only RSelenium, there is also possibility to use Docker to run the Selenium server and connect to this instance using RSelenium.

To setup the Selenium server and browser you need to use `rsDriver()` function and call $client to create the client. rsDriver functions expects several arguments:

* browser - I use chrome so I input "chrome",
* chromever - this is often the cause of error. I will go through the solution is separate blog, but in this case I input Chrome version "113.0.5672.63",
* verbose - I dont want any status messages so it is FALSE,
* port - port to run on. I use netstat package function `free_port(random = TRUE)` to automatically select free port

Afterwards I open the browser, maximizing the window, navigate to Nehnutelnosti page and accept cookies.
When all this is done, the actual scraping runs in for loop. I will not go through all details, but the logic is quite simple: 
1. Navigate to the link in the loop. 
2. Scroll to page height/10*4.2 so the javascript returning index of living will run (this scroll depth is based on manual testing, nevertheless I need to find other method or run multiple scrolls to really execute the script on all loaded pages)
3. Wait for 3 seconds to execute the javascript and slow down the process
4. Read the page content
5. If page could not be read and is NA, add blank record to the dataframe
6. Otherwise read page html, scrape following information and bind the new rows to the dataframe:
  * info_text - full text from advertisement. Not used currently in ML model, but I plan to use NLP to obtain keywords/topics and create a word cloud in Shiny app
  * info_details - contains 4 variables and will be cleaned in following steps. Variables are delimited by "\n".
  * index_of_living - value from 0 to 10, it is calculated by slovak startup [City Performer](https://cityperformer.com/). It considers six categories: environment, quality of housing, safety, transport, services and relaxation
  * additional_characteristics - contains multiple variables, I select 12 of them in following steps. Variables are delimited by "\n".

7. Close client and stop server

<br>
```{r RSelenium_scraping}
for (i in 1:10) {
  # get the current dataframe
  current_df <- get(paste0("advertisements_", i))

  # start the server
  rs_driver_object <- rsDriver(
    browser = "chrome",
    chromever = "113.0.5672.63",
    verbose = FALSE,
    port = free_port(random = TRUE)
  )

  # create a client object
  remDr <- rs_driver_object$client

  # open a browser
  remDr$open()
  remDr$maxWindowSize()
  
  # navigate to a website
  remDr$navigate("https://www.nehnutelnosti.sk/")
  Sys.sleep(5) # wait for 5 seconds

  # accept cookies
  remDr$switchToFrame(remDr$findElement(using = "xpath", '//*[@id="sp_message_iframe_710573"]'))
  remDr$findElement(using = "xpath", '//*[@id="notice"]/div[5]/div[2]/button')$clickElement()

  # loop through each link in the current dataframe
  for (link in current_df$link) {
    info_text <- NA
    additional_characteristics <- NA
    index_of_living <- NA
    info_details <- NA

    navigate_with_retry(link, remDr)
    height <- as.numeric(remDr$executeScript("return document.documentElement.scrollHeight"))/10*4.2 # Scroll to load index of living
    remDr$executeScript(paste("window.scrollTo(0, ", height, ");")) # scroll to living index
    
    Sys.sleep(3)
    page <- safe_find_element(remDr, '//*[@id="map-filter-container"]')

    if (is.na(page)) {
      new_row <- tibble(
        link = link,
        info_text = NA,
        additional_characteristics = NA,
        index_of_living = NA,
        info_details = NA
      )
      # bind new row to additional info dataframe
      additional_info_df <- rbind(additional_info_df, new_row)
    } else {
      page_html <- page$getElementAttribute("outerHTML")
      page_html <- read_html(page_html[[1]])

      info_text <- page_html %>%
        html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "text-inner", " " ))]') %>%
        get_text_or_na()

      info_details <- page_html %>%
        html_nodes(xpath = '//*[@id="map-filter-container"]/div[2]/div/div[1]/div[2]/div[5]/ul') %>%
        html_text2()
      
      tryCatch(
        {
          index_of_living <- page_html %>%
            html_nodes(xpath = '//*[@id="totalCityperformerWrapper"]/div/p[1]/span') %>%
            get_text_or_na()
        },
        error = function(e) {
          index_of_living <- NA
        }
      )

      tryCatch(
        {
          additional_characteristics <- page_html %>%
            html_nodes(xpath = '//*[@id="additional-features-modal-button"]/ul') %>%
            html_text2() # %>%
          # str_squish()
        },
        error = function(e) {
          additional_characteristics <- NA
        }
      )

      new_row <- tibble(
        link = link, info_text = info_text,
        additional_characteristics = additional_characteristics,
        index_of_living = index_of_living,
        info_details = info_details
      )

      # bind new row to additional info dataframe
      additional_info_df <- rbind(additional_info_df, new_row)
    }
  }

  # close client and stop server
  rs_driver_object$client$close()
  rs_driver_object$server$stop()
  rm(rs_driver_object, remDr)
  gc()
}
```

### Data wrangling
<br>
Scraping is done and I continue with data wrangling to adjust the data to usable form. 
Firstly, I deal with address to keep all districts in first column. This address is later used for geocoding. I also adjust the price to integer so it can be used in ML model.
<br>
```{r data_wrangling}
advertisements_cleaned <- advertisements %>%
  separate(type_of_real_estate, c("type", "area"), sep = " • ", remove = TRUE) %>% 
  separate(address, c("a", "b", "c"), sep = ", ", remove = TRUE) %>%
  unite("address", c(5, 4, 3), sep = ", ", na.rm = TRUE, remove = TRUE) %>% # reordering to keep all districts in first column
  mutate(
    price = str_replace_all(str_replace_all(price, " €", ""), " ", "") %>%
      as.integer(),
    address0 = address
  ) %>%
  separate(address0, c("district", "municipality", "street"), sep = ", ") %>%
  select(-street)
```
<br>
Now, I need to split the additional_characteristics and info_details columns values into more meaningful variables. For this I created two lists: characteristics1 and characteristics2. Each of them contains name of the variable I want to extract. I use these lists to create empty dataframe to ensure that all columns are present.
From additional_info_df (dataframe with scraped data) I select additional_characteristics and info_details and slit the values using "/n" as delimiter.
Next I define two functions: get_characteristics1 and get_characteristics2 which retrieve the values and map these functions to respective columns.
Finally I bind together output_df_characteristics1, output_df_characteristics2 and selected columns from additional_info_df and join them to advertisements_cleaned by link.
<br>
```{r}
# get additional information from scraped data
# First list of additional info details
characteristics1 <- c(
  "Stav",
  "Úžit. plocha",
  "Energie",
  "Provízia zahrnutá v cene"
)

characteristics1_df <- data.frame(characteristics1, value = NA)
# Second list of additional info details
characteristics2 <- c(
  "Počet izieb/miestností",
  "Orientácia",
  "Rok výstavby",
  "Rok poslednej rekonštrukcie",
  "Energetický certifikát",
  "Počet nadzemných podlaží",
  "Podlažie",
  "Výťah",
  "Typ konštrukcie",
  "Počet balkónov",
  "Počet lodžií",
  "Pivnica"
)

characteristics2_df <- data.frame(characteristics2, value = NA)

characteristics_wrangler <- additional_info_df %>%
  mutate(
    chars1_list = str_split(info_details, "\n"),
    chars2_list = str_split(additional_characteristics, "\n")
  ) %>%
  select(-additional_characteristics, -info_details)

get_characteristics1 <- function(x) {
  temp_df <- x %>%
    unlist() %>%
    as.data.frame()
  temp_df <- rename(temp_df, chars = .)
  temp_df <- temp_df %>%
    separate_wider_delim(chars,
      delim = ": ",
      names = c(
        "info",
        "status"
      )
    ) %>%
    filter(info %in% characteristics1) %>%
    full_join(characteristics1_df, join_by("info" == "characteristics1"), keep = FALSE) %>%
    select(-value) %>%
    pivot_wider(names_from = info, values_from = status)
  return(temp_df)
}

get_characteristics2 <- function(x) {
  temp_df <- x %>%
    unlist() %>%
    as.data.frame()
  temp_df <- rename(temp_df, chars = .)
  temp_df <- temp_df %>%
    separate_wider_delim(chars,
      delim = ": ",
      names = c(
        "info",
        "status"
      )
    ) %>%
    filter(info %in% characteristics2) %>%
    full_join(characteristics2_df, join_by("info" == "characteristics2"), keep = FALSE) %>%
    select(-value) %>%
    pivot_wider(names_from = info, values_from = status)
  return(temp_df)
}

# Apply get_characteristics1() and get_characteristics2() to each row in additional_info_df and combine the results
output_df_characteristics1 <- map_dfr(characteristics_wrangler$chars1_list, get_characteristics1)
output_df_characteristics2 <- map_dfr(characteristics_wrangler$chars2_list, get_characteristics2)

# Add the new columns to additional_info_df
additional_info_df_complete <- cbind(
  additional_info_df %>%
    mutate(index_of_living = str_replace_all(index_of_living, " /", "")) %>%
    select(c(link, info_text, index_of_living)) %>% 
    mutate(flag = "x"), 
  output_df_characteristics1,
  output_df_characteristics2
)
advertisements_complete <- advertisements_cleaned %>%
  left_join(additional_info_df_complete, by = "link", multiple = "first") %>%
  filter(!is.na(flag)) %>% 
  select(-flag, -c, -type)
```
<br>
Last step is saving the data in RDS format. I also keep the historical data for future price development analysis.
<br>
```{r save_data}
saveRDS(additional_info_df, "data/additional_info_df.RDS")
saveRDS(advertisements, "data/advertisements.RDS")

# save the old file advertisements_complete to histo folder for further use in web app
if (file.exists("data/advertisements_complete.rds")) {
  histo_rds <- import("data/advertisements_complete.rds") %>%
    mutate(timestamp = as.Date(file.info("data/advertisements_complete.rds")$mtime))
  histo_date <- file.info("data/advertisements_complete.rds")$mtime %>%
    as.Date() %>%
    as.character() %>%
    str_replace_all("-", "_")
  saveRDS(histo_rds, paste0("data/histo/advertisements_complete", histo_date, ".rds"))
}

# create separate df for text analyses
text_long <- advertisements_complete$info_text

# save the old file text_long to histo folder for further use in web app
if (file.exists("data/text_long.rds")) {
  histo_rds <- import("data/text_long.rds") %>%
    as.data.frame() %>% 
    mutate(timestamp = as.Date(file.info("data/text_long.rds")$mtime))
  histo_date <- file.info("data/text_long.rds")$mtime %>%
    as.Date() %>%
    as.character() %>%
    str_replace_all("-", "_")
  saveRDS(histo_rds, paste0("data/histo/text_long", histo_date, ".rds"))
}

saveRDS(text_long, file = "data/text_long.rds")
saveRDS(advertisements_complete, file = "data/advertisements_complete.rds")
advertisements_complete <- read_rds("data/advertisements_complete.rds")
```

