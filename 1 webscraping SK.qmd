---
title: Webscraping - ako použiť voľne dostupné dáta pre vlastnú analýzu
author: Arnold Kakas
format: html
date: today
theme: 
  - flatly
  - Style.scss
toc: true
toc-depth: 4
code-fold: true
execute:
  warning: false  
  echo: false
---

## Caution
<br>
Predtým než sa dostanem k téme tohto blogu, upozorňujem, že tento článok slúži výhradne <strong>informačným účelom</strong> a akékoľvek informácie uvedené nižšie <strong>nie sú právne rady</strong>. Preto pred akýmkoľvek zbieraním údajov z webu by ste mali získať vhodnú profesionálnu právnu radu týkajúcu sa vášho konkrétneho prípadu.

## Web scraping
<br>
V tomto blogovom príspevku prejdem procesom <strong>[web scraping-u](https://en.wikipedia.org/wiki/Web_scraping)</strong> s využitím <strong>programovacieho jazyka R</strong>. Predtým než sa pustím do samotného procesu, chcel by som sa trochu zamyslieť nad celkovou tématikou.
<br>
Web scraping je proces získavania obsahu alebo (väčšinou) štruktúrovaných údajov z webových stránok automatizovaným spôsobom (a obvykle vo veľkom množstve). Táto definícia prirodzene vyvoláva otázku o legalite takéhoto procesu.
V zásade web scraping nie je ilegálny alebo zakázaný sám osebe (v EÚ, k júlu 2023). Avšak, používanie nástrojov na ťaženie údajov je z právneho hľadiska riskantné z niekoľkých dôvodov:

* Porušenie duševného vlastníctva
* Porušenie zmluvy
* Obavy o ochranu osobných údajov

Pre zminimalizovanie obáv by malo scrapovanie prebiehať diskrétne, rešpektovať podmienky používania webových stránok, skontrolovať, či stránky používajú protokol robots.txt na oznámenie, že scrapovanie je zakázané, vyhnúť sa scrapovaniu osobných údajov a, ak je to nevyhnutné, uistiť sa, že nedochádza k porušeniu GDPR, a vyhnúť sa scrapovaniu súkromných alebo utajovaných informácií ([Zdroj](https://discoverdigitallaw.com/is-web-scraping-legal-short-guide-on-scraping-under-the-eu-jurisdiction/#IV_HOW_TO_SCRAPE_AND_NOT_BE_SUED)).
<br>
Existujú niektoré všeobecné etické zásady, ktoré by ste mali dodržiavať, keď chcete scrapovať údaje z webu. Najčastejšie spomínané sú:

* Ak existuje verejné <strong>API</strong>, ktoré poskytuje požadované údaje, použite ho namiesto scrapovania.
* Stahujte údaje v rozumnom tempe, aby scrapovanie nebolo škodlivé pre server a nemohlo byť zamieňané za <strong>DDoS útok</strong>. 
* Rešpektujte duševné vlastníctvo iných. Použite údaje na vytvorenie nového hodnotného obsahu, nie na duplikovanie a predávanie ich ako vlastné alebo nelegálne predávanie.
* Nepoužívajte scrapovanie osobných alebo súkromných údajov alebo dokumentov, rešpektujte GDPR.
* Skontrolujte súbor robots.txt, aby ste zistili, ako by mal byť web prehľadávaný.
* Zdieľajte to, čo môžete. Ak sú údaje, ktoré ste scrapovali, verejne dostupné, alebo ste získali povolenie na ich zdieľanie, zverejnite ich pre iných (napríklad na [GitHub](github.com) alebo [Kaggle](kaggle.com). Ak ste napísali webový scraper na prístup k nim, zdieľajte jeho kód, aby z neho mohli ťažiť aj iní.
* Hľadajte spôsoby, ako vrátiť hodnotu webovým stránkam, ktoré scrapujete, napríklad odkazovaním na stránku v článku alebo príspevku, aby ste na ňu priviedli návštevníkov.

Niekoľko článkov venujúcim sa téme môžete nájsť na [towardsdatascience.com](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01), alebo [Data Fluency github page](https://monashdatafluency.github.io/python-web-scraping/section-5-legal-and-ethical-considerations/), alebo [scrapingrobot page](https://scrapingrobot.com/blog/ethical-web-scraping/).
<br>
V tomto článku budem scrapovať údaje zo stránky [Nehnutelnosti](https://www.nehnutelnosti.sk/), ktorá sa špecializuje na realitné inzeráty a služby. Pre dodržanie etických zásad scrapovania som prijal niekoľko preventívnych opatrení: 

* Táto konkrétna stránka používa protokol robots.txt. Nescrapujem žiadnu časť, ktorá je zakázaná, a vo vybraných častiach kódu som pridal funkciu _Sys.sleep()_ aby som spomalil proces a žiadal údaje s primeranou frekvenciou.
* Scrapujem len verejné údaje, ktoré potrebujem na ďaľšiu analýzu a vytvorenie <strong>ML modelu</strong>.
* Mám verejne dostupný [GitHub repo](https://github.com/Arnold-Kakas/real_estate_appraisal), kde je možné pristupovať ku všetkému kódu a údajom a výsledný dataset bude dostupný aj na [Kaggle](https://www.kaggle.com/datasets/arnoldkakas/real-estate-dataset).
<br>
<center>
![Nehnutelnosti website robots.txt protocol](data/Nehnutelnosti robots protocol.JPG)
</center>
<br>

## Proces scrapovania {.tabset .tab-pills}
<br>
### Použité knižnice
<br>
Ako obvykle, začínam s načítaním balíkov potrebných pre tento projekt. Používam na to [packman](https://cran.r-project.org/web/packages/pacman/index.html) knižnicu a funkciu _p_load()_, ktorá ma dve výhody oproti základnej funkcii _library()_:

* ak potrebnú knižnicu nemám nainstalovanú, funkcia to rovno napraví
* môžem načítať viacero knižníc naraz

```{r}
if (!require("pacman")) {
  install.packages("pacman")
}
pacman::p_load(
  rio,
  tidyverse,
  rvest, # scraping, part of tidyverse
  httr, # working with html
  RSelenium, # scraping in Google Chrome
  netstat, # free_port()
  doParallel, # parallel processing
  furrr # future map
)
```


V tomto projekte máme tri skupiny balíkov:

* na načítanie údajov a manipuláciu s nimi - rio, tidyverse
* na web scrapovanie - rvest, RSelenium, netstat, httr
* na paralelné spracovanie - doParallel a furrr

### Webové elementy
<br>
Bez ohľadu na to, ktorý programovací jazyk alebo balík si vyberiete na scrapovanie, musíte byť schopní nájsť elementy v zdrojovom kóde webovej stránky. To môžete jednoducho urobiť vo vašom webovom prehliadači (pre potreby tohto blogu používam Google Chrome). Stlačte  *CRTL* + *SHIFT* + *I* na otvorenie nástrojov vývojára. Teraz, keď sa pohybujete kurzorom po kóde v okne, dynamicky vám ukáže, ktorá časť stránky je s ňou spojená. Jednoduchším spôsobom, ako získať správnu referenciu na element (alebo inú časť stránky), je stlačiť *CRTL* + *SHIFT* + *C* a vybrať priamo na stránke požadovaný element.
<br>
<center>
![DevTools v Google Chrome](data/DevTools.JPG)
</center>
<br>

Ak ste našli správny element, musíte skopírovať jeho CSS selektor alebo cestu XPath. Obe možnosti môžu byť použité ako argumenty v rvest a RSelenium.
<br>
<center>
![Kopírovanie XPath web elementu](data/Web element Xpath-CSS selector.JPG)
</center>
<br>

Teraz ste pripravení získať obsah zo stránky. Výsledok, ak ste všetko správne urobili, môže mať rôzne formy. Môže to byť jedna hodnota, reťazec, zoznam atď. Na základe toho buď presnejšie špecifikujete, ktorú časť obsahu potrebujete, alebo pracujete s výsledkom v R a používate funkcie na manipuláciu s dátami, aby ste získali požadované informácie.

### Scraping časť I. - rvest
<br>
Jedným z najbežnejších balíkov na web scrapovanie v R je [rvest](https://rvest.tidyverse.org/). Poskytuje funkcie na prístup k verejnej webovej stránke a na vyhľadávanie špecifických prvkov pomocou selektorov CSS a XPath. Tento balík neodpaluje javascript, čo znamená, že načíta html stránky rýchlejšie, ale vynechá všetky prvky načítané javascriptom po pôvodnom načítaní stránky. Preto je tento balík dobrá voľba, ak scrapujete statické stránky.

V tomto príklade začínam vytvorením premennej pre zdrojovú URL adresu: *https://www.nehnutelnosti.sk/slovensko/byty/predaj/?p[param1][from]=1000&p[param1][to]=&p[page]=*

Následne skontrolujem počet stránok:
<br>
<center>
![Number of pages with search results](data/Number of pages.jpg)
</center>
<br>
A nakoniec pripravím a spustím multisession na získanie ceny, adresy, typu nehnuteľnosti a najdôležitejšie - odkazu na inzerát, ktorý bude použitý v ďalšej časti s balíkom RSelenium.
Pridal som aj _plan(sequential)_ na zastavenie multisession, ale musím priznať, že nie som tak znalý paralelného programovania v R, aby som úplne pochopil dôležitosť tohto kroku.
<br>
```{r}
# apartments page
site <- "https://www.nehnutelnosti.sk/slovensko/byty/predaj/?p[param1][from]=1000&p[param1][to]=&p[page]="

# scrape the number of pages
number_of_pages <- read_html(paste0(site, 1)) %>%
  html_nodes(xpath = '//*[@id="content"]/div[8]/div/div/div[1]/div[17]/div/div/ul/li[5]') %>%
  html_elements("a") %>%
  html_text(trim = TRUE) %>%
  as.numeric()

# create a cluster of worker processes (cores)
plan(multisession, workers = 6)

advertisements <- future_map_dfr(1:number_of_pages, function(i) {
  page_content <- read_html(paste0(site, i))
  
  price <- page_content %>%
    html_nodes(xpath = '//*[@class="advertisement-item--content__price col-auto pl-0 pl-md-3 pr-0 text-right mt-2 mt-md-0 align-self-end"]') %>%
    html_attr("data-adv-price")
  
  type_of_real_estate <- page_content %>%
    html_nodes(xpath = '//*[@class="advertisement-item--content__info"]') %>%
    html_text2()
  
  address <- page_content %>%
    html_nodes(xpath = '//*[@class="advertisement-item--content__info d-block text-truncate"]') %>%
    html_text2()
  
  link <- page_content %>%
    html_nodes(xpath = '//*[@class="mb-0 d-none d-md-block"]') %>%
    html_nodes("a") %>%
    html_attr("href")
  
  tibble(price = price, type_of_real_estate = type_of_real_estate, address = address, link = link)
})

plan(sequential)
```

### Scraping časť II. - RSelenium
<br>
[RSelenium](https://cran.r-project.org/web/packages/RSelenium/index.html) Poskytuje súbor R väzieb pre *Selenium 2.0 WebDriver*. Na rozdiel od rvest spúšťa skutočný webový prehliadač, takže načíta akýkoľvek javascript obsiahnutý na webovej stránke.
S týmto balíkom budete schopní interagovať so stránkou, napríklad posúvať, klikať na tlačidlo, vyplňovať vstupné formuláre atď.
Na druhej strane je použitie tohto balíka náročnejšie, vyžaduje inštalovaný jazyk <strong>Java</strong> vo vašom systéme, a ja som sa stretol s viacerými problémami, kým som ho správne spustil. Viac o tomto téme bude pokryté v jednom z budúcich blogov.
<br>
I start with defining several helper functions. 
One is for unsuccessful navigation to page, this usually happened when I read too many pages and I had to delete browsing history. Sometimes this function created loop running several hours before I noticed (e.g. during night) but at least the code did crashed. I also tried creating function to delete the history in case of unsuccessful navigation, but it did not work and I am fine with this solution for the time being. 
Second one is for handling error in element search. In such cases this function returns NA.
Last one is similar to the previous, however in this case it returns NA in case when element is not found.
<br>
I also use `tryCatch()` function in scraping script to return NA in case of error.
<br>
```{r}
# Define a function that handles the errors in page load
navigate_with_retry <- function(link, remDr) {
  success <- FALSE
  while (!success) {
    tryCatch(
      {
        remDr$navigate(link)
        Sys.sleep(5)
        success <- TRUE
      },
      error = function(e) {
        cat("Failed to navigate to", link, "- Retrying in 10 seconds...\n")
        clearCookies(remDr)
        Sys.sleep(10)
      }
    )
  }
}

# Define a wrapper function that handles the errors in element search
safe_find_element <- possibly(function(page, xpath) {
  page$findElement(using = "xpath", xpath)
}, NA)

# function to get text or return NA if element not found
get_text_or_na <- function(nodes) {
  tryCatch(
    {
      text <- nodes %>%
        html_text2() %>%
        as.character() # %>%
      # str_trim() %>%
      # str_squish()
      if (text == "") NA else text
    },
    error = function(e) {
      NA
    }
  )
}
```
<br>
Now I am splitting links to 10 batches. This does not has to be done since the code is working, but I keep it in case of some unexpected error occurrence during the scraping process in order to save at least part of the data. This may seem paranoid, but I learned to be causes the hard way. The next part of web scraping takes more than two days and you don't want to lose whole progress because of WiFi outage. 
I am also creating empty dataframe with all possible columns which will be fed later in the process.
<br>
```{r}
# Additional info from web

# number of splits
num_splits <- 10
split_size <- ceiling(nrow(advertisements) / num_splits)

# split the data frame into subsets
advertisments_list <- split(advertisements, rep(1:num_splits, each = split_size, length.out = nrow(advertisements)))

for (i in seq_along(advertisments_list)) {
  assign(paste0("advertisements_", i), advertisments_list[[i]])
}

# create empty dataframe outside of the loop to hold additional info
additional_info_df <- tibble(
  link = character(),
  info_text = character(),
  additional_characteristics = character(),
  index_of_living = character(),
  info_details = character(),
  stringsAsFactors = FALSE
)

```
<br>
Now I am getting to actual scraping with RSelenium. As already mentioned, to get it running I need to follow several steps. I am using only RSelenium, there is also possibility to use Docker to run the Selenium server and connect to this instance using RSelenium.

To setup the Selenium server and browser you need to use `rsDriver()` function and call $client to create the client. rsDriver functions expects several arguments:

* browser - I use chrome so I input "chrome",
* chromever - this is often the cause of error. I will go through the solution is separate blog, but in this case I input Chrome version "113.0.5672.63",
* verbose - I dont want any status messages so it is FALSE,
* port - port to run on. I use netstat package function `free_port(random = TRUE)` to automatically select free port

Afterwards I open the browser, maximizing the window, navigate to Nehnutelnosti page and accept cookies.
When all this is done, the actual scraping runs in for loop. I will not go through all details, but the logic is quite simple: 
1. Navigate to the link in the loop. 
2. Scroll to page height/10*4.2 so the javascript returning index of living will run (this scroll depth is based on manual testing, nevertheless I need to find other method or run multiple scrolls to really execute the script on all loaded pages)
3. Wait for 3 seconds to execute the javascript and slow down the process
4. Read the page content
5. If page could not be read and is NA, add blank record to the dataframe
6. Otherwise read page html, scrape following information and bind the new rows to the dataframe:
  * info_text - full text from advertisement. Not used currently in ML model, but I plan to use NLP to obtain keywords/topics and create a word cloud in Shiny app
  * info_details - contains 4 variables and will be cleaned in following steps. Variables are delimited by "\n".
  * index_of_living - value from 0 to 10, it is calculated by slovak startup [City Performer](https://cityperformer.com/). It considers six categories: environment, quality of housing, safety, transport, services and relaxation
  * additional_characteristics - contains multiple variables, I select 12 of them in following steps. Variables are delimited by "\n".
7. Close client and stop server

<br>
```{r}
for (i in 1:10) {
  # get the current dataframe
  current_df <- get(paste0("advertisements_", i))

  # start the server
  rs_driver_object <- rsDriver(
    browser = "chrome",
    chromever = "113.0.5672.63",
    verbose = FALSE,
    port = free_port(random = TRUE)
  )

  # create a client object
  remDr <- rs_driver_object$client

  # open a browser
  remDr$open()
  remDr$maxWindowSize()
  
  # navigate to a website
  remDr$navigate("https://www.nehnutelnosti.sk/")
  Sys.sleep(5) # wait for 5 seconds

  # accept cookies
  remDr$switchToFrame(remDr$findElement(using = "xpath", '//*[@id="sp_message_iframe_710573"]'))
  remDr$findElement(using = "xpath", '//*[@id="notice"]/div[5]/div[2]/button')$clickElement()

  # loop through each link in the current dataframe
  for (link in current_df$link) {
    info_text <- NA
    additional_characteristics <- NA
    index_of_living <- NA
    info_details <- NA

    navigate_with_retry(link, remDr)
    height <- as.numeric(remDr$executeScript("return document.documentElement.scrollHeight"))/10*4.2 # Scroll to load index of living
    remDr$executeScript(paste("window.scrollTo(0, ", height, ");")) # scroll to living index
    
    Sys.sleep(3)
    page <- safe_find_element(remDr, '//*[@id="map-filter-container"]')

    if (is.na(page)) {
      new_row <- tibble(
        link = link,
        info_text = NA,
        additional_characteristics = NA,
        index_of_living = NA,
        info_details = NA
      )
      # bind new row to additional info dataframe
      additional_info_df <- rbind(additional_info_df, new_row)
    } else {
      page_html <- page$getElementAttribute("outerHTML")
      page_html <- read_html(page_html[[1]])

      info_text <- page_html %>%
        html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "text-inner", " " ))]') %>%
        get_text_or_na()

      info_details <- page_html %>%
        html_nodes(xpath = '//*[@id="map-filter-container"]/div[2]/div/div[1]/div[2]/div[5]/ul') %>%
        html_text2()
      
      tryCatch(
        {
          index_of_living <- page_html %>%
            html_nodes(xpath = '//*[@id="totalCityperformerWrapper"]/div/p[1]/span') %>%
            get_text_or_na()
        },
        error = function(e) {
          index_of_living <- NA
        }
      )

      tryCatch(
        {
          additional_characteristics <- page_html %>%
            html_nodes(xpath = '//*[@id="additional-features-modal-button"]/ul') %>%
            html_text2() # %>%
          # str_squish()
        },
        error = function(e) {
          additional_characteristics <- NA
        }
      )

      new_row <- tibble(
        link = link, info_text = info_text,
        additional_characteristics = additional_characteristics,
        index_of_living = index_of_living,
        info_details = info_details
      )

      # bind new row to additional info dataframe
      additional_info_df <- rbind(additional_info_df, new_row)
    }
  }

  # close client and stop server
  rs_driver_object$client$close()
  rs_driver_object$server$stop()
  rm(rs_driver_object, remDr)
  gc()
}
```

### Data wrangling
<br>
Scraping is done and I continue with data wrangling to adjust the data to usable form. 
Firstly, I deal with address to keep all districts in first column. This address is later used for geocoding. I also adjust the price to integer so it can be used in ML model.
<br>
```{r}
advertisements_cleaned <- advertisements %>%
  separate(type_of_real_estate, c("type", "area"), sep = " • ", remove = TRUE) %>% 
  separate(address, c("a", "b", "c"), sep = ", ", remove = TRUE) %>%
  unite("address", c(5, 4, 3), sep = ", ", na.rm = TRUE, remove = TRUE) %>% # reordering to keep all districts in first column
  mutate(
    price = str_replace_all(str_replace_all(price, " €", ""), " ", "") %>%
      as.integer(),
    address0 = address
  ) %>%
  separate(address0, c("district", "municipality", "street"), sep = ", ") %>%
  select(-street)
```
<br>
Now, I need to split the additional_characteristics and info_details columns values into more meaningful variables. For this I created two lists: characteristics1 and characteristics2. Each of them contains name of the variable I want to extract. I use these lists to create empty dataframe to ensure that all columns are present.
From additional_info_df (dataframe with scraped data) I select additional_characteristics and info_details and slit the values using "/n" as delimiter.
Next I define two functions: get_characteristics1 and get_characteristics2 which retrieve the values and map these functions to respective columns.
Finally I bind together output_df_characteristics1, output_df_characteristics2 and selected columns from additional_info_df and join them to advertisements_cleaned by link.
<br>
```{r}
# get additional information from scraped data
# First list of additional info details
characteristics1 <- c(
  "Stav", # condition
  "Úžit. plocha", # land area
  "Energie", # energy costs
  "Provízia zahrnutá v cene"
)

characteristics1_df <- data.frame(characteristics1, value = NA)
# Second list of additional info details
characteristics2 <- c(
  "Počet izieb/miestností", # number of rooms
  "Orientácia", # orientation
  "Rok výstavby", # built year
  "Rok poslednej rekonštrukcie", # year of last reconstruction
  "Energetický certifikát", # energy certificate
  "Počet nadzemných podlaží", # number of floors
  "Podlažie", # floor
  "Výťah", # lift
  "Typ konštrukcie", # construction type
  "Počet balkónov", # number of balconies
  "Počet lodžií", # number of loggias
  "Pivnica" # cellar
)

characteristics2_df <- data.frame(characteristics2, value = NA)

characteristics_wrangler <- additional_info_df %>%
  mutate(
    chars1_list = str_split(info_details, "\n"),
    chars2_list = str_split(additional_characteristics, "\n")
  ) %>%
  select(-additional_characteristics, -info_details)

get_characteristics1 <- function(x) {
  temp_df <- x %>%
    unlist() %>%
    as.data.frame()
  temp_df <- rename(temp_df, chars = .)
  temp_df <- temp_df %>%
    separate_wider_delim(chars,
      delim = ": ",
      names = c(
        "info",
        "status"
      )
    ) %>%
    filter(info %in% characteristics1) %>%
    full_join(characteristics1_df, join_by("info" == "characteristics1"), keep = FALSE) %>%
    select(-value) %>%
    pivot_wider(names_from = info, values_from = status)
  return(temp_df)
}

get_characteristics2 <- function(x) {
  temp_df <- x %>%
    unlist() %>%
    as.data.frame()
  temp_df <- rename(temp_df, chars = .)
  temp_df <- temp_df %>%
    separate_wider_delim(chars,
      delim = ": ",
      names = c(
        "info",
        "status"
      )
    ) %>%
    filter(info %in% characteristics2) %>%
    full_join(characteristics2_df, join_by("info" == "characteristics2"), keep = FALSE) %>%
    select(-value) %>%
    pivot_wider(names_from = info, values_from = status)
  return(temp_df)
}

# Apply get_characteristics1() and get_characteristics2() to each row in additional_info_df and combine the results
output_df_characteristics1 <- map_dfr(characteristics_wrangler$chars1_list, get_characteristics1)
output_df_characteristics2 <- map_dfr(characteristics_wrangler$chars2_list, get_characteristics2)

# Add the new columns to additional_info_df
additional_info_df_complete <- cbind(
  additional_info_df %>%
    mutate(index_of_living = str_replace_all(index_of_living, " /", "")) %>%
    select(c(link, info_text, index_of_living)) %>% 
    mutate(flag = "x"), 
  output_df_characteristics1,
  output_df_characteristics2
)
advertisements_complete <- advertisements_cleaned %>%
  left_join(additional_info_df_complete, by = "link", multiple = "first") %>%
  filter(!is.na(flag)) %>% 
  select(-flag, -c, -type)
```
<br>
Last step is saving the data in RDS format. I also keep the historical data for future price development analysis.
<br>
```{r}
saveRDS(additional_info_df, "data/additional_info_df.RDS")
saveRDS(advertisements, "data/advertisements.RDS")

# save the old file advertisements_complete to histo folder for further use in web app
if (file.exists("data/advertisements_complete.rds")) {
  histo_rds <- import("data/advertisements_complete.rds") %>%
    mutate(timestamp = as.Date(file.info("data/advertisements_complete.rds")$mtime))
  histo_date <- file.info("data/advertisements_complete.rds")$mtime %>%
    as.Date() %>%
    as.character() %>%
    str_replace_all("-", "_")
  saveRDS(histo_rds, paste0("data/histo/advertisements_complete", histo_date, ".rds"))
}

# create separate df for text analyses
text_long <- advertisements_complete$info_text

# save the old file text_long to histo folder for further use in web app
if (file.exists("data/text_long.rds")) {
  histo_rds <- import("data/text_long.rds") %>%
    as.data.frame() %>% 
    mutate(timestamp = as.Date(file.info("data/text_long.rds")$mtime))
  histo_date <- file.info("data/text_long.rds")$mtime %>%
    as.Date() %>%
    as.character() %>%
    str_replace_all("-", "_")
  saveRDS(histo_rds, paste0("data/histo/text_long", histo_date, ".rds"))
}

saveRDS(text_long, file = "data/text_long.rds")
saveRDS(advertisements_complete, file = "data/advertisements_complete.rds")
advertisements_complete <- read_rds("data/advertisements_complete.rds")
```

