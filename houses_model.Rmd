---
title: "houses_model"
author: "Arnold Kaka≈°"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
# Display only results in output
knitr::opts_chunk$set(echo = FALSE)
Sys.setenv(RETICULATE_PYTHON = "C:/Users/kakas/AppData/Local/r-miniconda/")
# Load pacman library for managing packages
library(pacman)

# Load and install multiple packages at once
p_load(rio, tidyverse, tidymodels, GGally, mice, vip, reticulate, skimr, spacyr)

# Prefer tidymodels package functions
tidymodels_prefer()

# specify Python
use_python("C:/Users/kakas/AppData/Local/r-miniconda/python.exe", required = TRUE)
```

```{python}
# Load Python modules
import multiprocessing as mp
import pandas as pd
import spacy
import time
from googletrans import Translator
import rpy2.rinterface
from collections import defaultdict
```

create and evaluate models


```{r}
# Import data
houses <- rio::import("data/houses.rds")
text <- rio::import("data/texts.rds")
```


```{r}
# Merge text to houses db
houses_merged <- houses %>%
  left_join(text, by = c("link" = "url"))
```


```{r warning=FALSE}
# Transform vectors types
houses_cleaned <- houses_merged %>%
  mutate(
    usable_area = as.numeric(usable_area),
    built_up_area = as.numeric(built_up_area),
    land_area = as.numeric(land_area)
  )
```


```{r warning=FALSE}
# impute missing values
set.seed(123)
houses_cleaned <-
  houses_cleaned %>%
  mice(m = 5, maxit = 5, method = "pmm", print = FALSE)
houses_cleaned <- complete(houses_cleaned)
```


```{r}
houses_cleaned <- houses_cleaned %>%
  filter(
    usable_area > quantile(usable_area, 0.01),
    usable_area < quantile(usable_area, 0.9),
    built_up_area < quantile(built_up_area, 0.9),
    land_area < quantile(land_area, 0.9)
  ) %>%
  mutate(
    condition = replace_na(condition, "not provided"),
    district = as_factor(district),
    municipality = as_factor(municipality),
    type = as_factor(type),
    commission_in_price = as_factor(commission_in_price)
  )
```

```{r}
houses_testing <- houses_cleaned %>% top_n(10, price)
```

```{python}
# Define the number of retries
num_retries = 5

# Define a function for translating text from Slovak to English
def translate_text(text):
    """
    Translates the input text from Slovak to English.

    Parameters
    ----------
    text : str
        The text to be translated.

    Returns
    -------
    str
        The translated text.
    """
    # Counter for the number of retries
    retry_count = 0
    # Result of the translation
    result = ""
    # Flag to indicate if the translation was successful
    success = False
    while not success and retry_count < num_retries:
        try:
            # Create a Translator object
            translator = Translator(service_urls=['translate.google.com'])
            # Translate the text
            result = translator.translate(text, dest='en').text
            # Wait 2 seconds before translating the next text
            time.sleep(2)
            # Log the input text and the result
            # print(f"Input text: {text}")
            # print(f"Result: {result}")
            # Set the success flag to True if the result is not None
            success = result is not None
        except Exception as e:
            print(f"Error occurred while translating text: {e}")
            retry_count += 1
    # Return an empty string if the result is None
    if result is None:
        return ""
    else:
        return result
```

```{python}
# Pass the "houses" dataframe from R to Python
houses_r = r.houses_testing

# Perform your modification on the Pandas dataframe
houses_r['info_text_en'] = houses_r['info_text'].apply(lambda x: translate_text(x))

```

```{python}
# Define a function to phrases/keywords from text
nlp = spacy.load("en_core_web_lg")

# Define the extract_main_phrases() function
def extract_main_phrases(text):
    doc = nlp(text)
    phrases = []
    for chunk in doc.noun_chunks:
        if not chunk.root.is_stop and not chunk.root.is_punct:
            phrases.append(chunk.lemma_.lower())
    for token in doc:
        if token.dep_ == 'ROOT' and token.pos_ == 'VERB' and not token.is_stop and not token.is_punct:
            phrases.append(token.lemma_.lower())
    return phrases

def get_similar_phrases(phrase, phrases):
    phrase_vec = nlp(phrase).vector
    phrase_vecs = [nlp(phrase).vector for phrase in phrases]
    similarities = nlp.vocab.vectors.cosine_similarities(phrase_vecs, [phrase_vec])
    top_indices = similarities.argsort()[:-6:-1]
    return [phrases[i] for i in top_indices]

def cluster_phrases(phrases):
    clusters = defaultdict(list)
    for i, phrase1 in enumerate(phrases):
        for j, phrase2 in enumerate(phrases[i+1:], i+1):
            sim = nlp(phrase1).similarity(nlp(phrase2))
            if sim > 0.8:
                clusters[i].append(j)
                clusters[j].append(i)
    groups = []
    seen = set()
    for i in range(len(phrases)):
        if i not in seen:
            group = [phrases[i]]
            seen.add(i)
            for j in clusters[i]:
                if j not in seen:
                    group.append(phrases[j])
                    seen.add(j)
            groups.append(group)
    return groups
```

```{python, results='asis'}
# Extract the main phrases for each row in the 'info_text_en' column
houses_r['main_phrases'] = houses_r['info_text_en'].apply(extract_main_phrases)
houses_r['grouped_phrases'] = houses_r['main_phrases'].apply(cluster_phrases)
```




 
```{r}
ggplot(houses_cleaned, mapping = aes(x = built_up_area)) +
 geom_boxplot()
```

```{r}
# EDA
summary(houses_cleaned)
```
 
```{r}
 ggpairs(houses_cleaned[, c(1, 6, 7, 10, 11)], progress = FALSE)
```

```{r}

houses_BoxCox <- recipe(houses_train, price ~ .) %>%
   step_rm(lon, lat, lon_rotated, lat_rotated) %>%
   step_BoxCox(all_numeric_predictors()) %>%
   prep() %>%
   bake(new_data = NULL)

 houses_log <- recipe(houses_train, price ~ .) %>%
   step_rm(lon, lat, lon_rotated, lat_rotated) %>%
   step_log(all_numeric_predictors()) %>%
   prep() %>%
   bake(new_data = NULL)

 houses_norm <- recipe(houses_train, price ~ .) %>%
   step_rm(lon, lat, lon_rotated, lat_rotated) %>%
   step_normalize(all_numeric_predictors()) %>%
   prep() %>%
   bake(new_data = NULL)

# Plot distribution of built_up_area before and after transformations
p1 <- ggplot(data = houses_cleaned, aes(x = built_up_area)) +
  geom_density() +
  ggtitle("price before transformations")

p2 <- ggplot(data = houses_BoxCox, aes(x = built_up_area)) +
  geom_density() +
  ggtitle("price after Box-Cox transform")

p3 <- ggplot(data = houses_norm, aes(x = built_up_area)) +
  geom_density() +
  ggtitle("price after normalization")

p4 <- ggplot(data = houses_log, aes(x = built_up_area)) +
  geom_density() +
  ggtitle("price after log transform")

 (p1 + p2) / (p3 + p4)
```



```{r}
split dataframes to train(80)/test(20)

houses_cleaned <- droplevels(houses_cleaned)
set.seed(345)
houses_train_split <- initial_split(houses_cleaned, prop = 0.8, strata = price)

houses_train <- training(houses_train_split)
houses_test <- testing(houses_train_split)

set.seed(567)
houses_train_boots <- bootstraps(houses_train, times = 25)
```

```{r}
# recipes
houses_xgboost_recipe <- recipe(houses_train, price ~ .) %>%
  step_rm(lat, lon, municipality) %>%
  step_naomit(all_nominal_predictors()) %>%
  step_BoxCox(usable_area, built_up_area, land_area) %>%
  step_other(all_nominal_predictors(), threshold = 0.01) %>%
  step_dummy(all_nominal_predictors())
```


```{r}
# models
xgb_model <-
  boost_tree(
    trees = 1000, loss_reduction = tune(),
    tree_depth = tune(), min_n = tune(),
    mtry = tune(), sample_size = tune(),
    learn_rate = tune()
  ) %>%
  set_mode("regression") %>%
  set_engine("xgboost", nthread = 6) # use 6 cores for multithreading
```

```{r}
# workflows
houses_workflow <- workflow() %>%
  add_recipe(houses_xgboost_recipe) %>%
  add_model(xgb_model)
```

```{r}
# parameters
houses_xgboost_params <- parameters(
  trees(),
  tree_depth(), min_n(),
  finalize(mtry(), houses_train)
)
```

```{r}
# optimization
set.seed(789)
houses_xgboost_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), houses_train),
  learn_rate(),
  size = 30
)
```

```{r}
# results
houses_xgboost_res <- tune_grid(
  houses_workflow,
  resamples = houses_train_boots,
  grid = houses_xgboost_grid,
  control = control_grid(save_pred = TRUE)
)
```

```{r}
# best models
houses_best_model <- select_best(houses_xgboost_res, "rmse")
```

```{r}
# finalize models
houses_final_model <- finalize_model(xgb_model, houses_best_model)
houses_workflow <- houses_workflow %>% update_model(houses_final_model)
houses_xgb_fit <- fit(houses_workflow, data = houses_train)
```

```{r}
houses_xgb_fit %>%
  fit(data = houses_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

```{r}
houses_final_res <- last_fit(houses_xgb_fit, houses_train_split)
collect_metrics(houses_final_res)
# rsq is small, need to look for more predictors
```

```{r}
# evaluate
houses_pred <-
  predict(houses_xgb_fit, houses_test) %>%
  bind_cols(houses_test) %>%
  mutate(
    prediction = 10^.pred,
    real_price = 10^price
  )

houses_plot1 <-
  houses_pred %>%
  ggplot(aes(x = .pred, y = price)) +
  geom_point() +
  geom_abline(intercept = 0, col = "red")


houses_plot2 <-
  houses_pred %>%
  select(.pred, price) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, volor = key, fill = key)) +
  geom_density(alpha = .2) +
  labs(x = "", y = "")

houses_plot1 / houses_plot2
```
