---
title: "houses_model"
author: "Arnold Kakaš"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
# Display only results in output
knitr::opts_chunk$set(echo = FALSE)
Sys.setenv(RETICULATE_PYTHON = "C:/Users/kakas/AppData/Local/r-miniconda/")
# Load pacman library for managing packages
library(pacman)

# Load and install multiple packages at once
p_load(rio, tidyverse, tidymodels, GGally, mice, vip, reticulate, skimr, spacyr)

# Prefer tidymodels package functions
tidymodels_prefer()

# specify Python
use_python("C:/Users/kakas/AppData/Local/r-miniconda/python.exe", required = TRUE)
```

```{python}
# Load Python modules
# import multiprocessing as mp
import pandas as pd
import spacy
import time
from googletrans import Translator
import rpy2.rinterface
from collections import defaultdict
from string import punctuation # Idea from Medium https://medium.com/better-programming/extract-keywords-using-spacy-in-python-4a8415478fbf
from collections import Counter # Idea from Medium https://medium.com/better-programming/extract-keywords-using-spacy-in-python-4a8415478fbf
```

create and evaluate models


```{r}
# Import data
houses <- rio::import("data/houses.rds")
text <- rio::import("data/texts.rds")
```


```{r}
# Merge text to houses db
houses_merged <- houses %>%
  left_join(text, by = c("link" = "url"))
```


```{r warning=FALSE}
# Transform vectors types
houses_cleaned <- houses_merged %>%
  mutate(
    usable_area = as.numeric(usable_area),
    built_up_area = as.numeric(built_up_area),
    land_area = as.numeric(land_area)
  )
```


```{r warning=FALSE}
# impute missing values
set.seed(123)
houses_cleaned <-
  houses_cleaned %>%
  mice(m = 5, maxit = 5, method = "pmm", print = FALSE)
houses_cleaned <- complete(houses_cleaned)
```


```{r}
houses_cleaned <- houses_cleaned %>%
  filter(
    usable_area > quantile(usable_area, 0.01),
    usable_area < quantile(usable_area, 0.9),
    built_up_area < quantile(built_up_area, 0.9),
    land_area < quantile(land_area, 0.9)
  ) %>%
  mutate(
    condition = replace_na(condition, "not provided"),
    district = as_factor(district),
    municipality = as_factor(municipality),
    type = as_factor(type),
    commission_in_price = as_factor(commission_in_price)
  )
```

```{r}
houses_testing <- houses_cleaned %>% top_n(10, price)
```

```{python}
# Define the number of retries
num_retries = 5

# Define a function for translating text from Slovak to English
def translate_text(text):
    """
    Translates the input text from Slovak to English.

    Parameters
    ----------
    text : str
        The text to be translated.

    Returns
    -------
    str
        The translated text.
    """
    # Counter for the number of retries
    retry_count = 0
    # Result of the translation
    result = ""
    # Flag to indicate if the translation was successful
    success = False
    while not success and retry_count < num_retries:
        try:
            # Create a Translator object
            translator = Translator(service_urls=['translate.google.com'])
            # Translate the text
            result = translator.translate(text, dest='en').text
            # Wait 2 seconds before translating the next text
            time.sleep(3)
            # Log the input text and the result
            # print(f"Input text: {text}")
            # print(f"Result: {result}")
            # Set the success flag to True if the result is not None
            success = result is not None
        except Exception as e:
            print(f"Error occurred while translating text: {e}")
            retry_count += 1
    # Return an empty string if the result is None
    if result is None:
        return ""
    else:
        return result
```

```{python}
# Pass the "houses" dataframe from R to Python
houses_r = r.houses_cleaned

# Perform your modification on the Pandas dataframe
houses_r['info_text_en'] = houses_r['info_text'].apply(lambda x: translate_text(x))

```

```{python}
# Define a function to phrases/keywords from text
nlp = spacy.load("en_core_web_lg")
```

```{python}
# Define the extract_main_phrases() function
def extract_main_phrases(text):
    """
    Extracts the main phrases from a text by identifying noun chunks and root verbs.

    Parameters:
        text (str): The input text to extract main phrases from.
        
    Returns:
        list: A list of the main phrases in the input text, where each phrase is represented as a lowercase string.
    """
    # Parse the input text using spaCy
    doc = nlp(text)
    
    # Initialize an empty list to store the main phrases
    phrases = []
    
    # Extract noun chunks and add them to the phrases list
    for chunk in doc.noun_chunks:
        # Check that the root word of the noun chunk is not a stop word or punctuation
        if not chunk.root.is_stop and not chunk.root.is_punct:
            phrases.append(chunk.lemma_.lower())
    
    # Extract root verbs and add them to the phrases list
    for token in doc:
        # Check that the token is a root verb and not a stop word or punctuation
        if token.dep_ == 'ROOT' and token.pos_ == 'VERB' and not token.is_stop and not token.is_punct:
            phrases.append(token.lemma_.lower())
    
    # Return the list of main phrases
    return phrases

```

```{python}
def get_hotwords(text):
    """
    Extracts the top 5 most common hot words from the input text after removing stop words and punctuations.
    
    Args:
        text (str): The input text to extract hot words from.
    
    Returns:
        List of strings: The top 5 most common hot words in the input text.
    """
    nlp.Defaults.stop_words |= {"•","m2", "house"}  # add additional stop words
    result = []
    doc = nlp(text.lower())
    pos_tag = ['PROPN', 'ADJ', 'NOUN']
    result = [token.text for token in doc if token.text not in nlp.Defaults.stop_words and token.text not in punctuation and token.pos_ in pos_tag]
    return [word for word, count in Counter(result).most_common(5)]

```

```{python, results='asis'}
# Extract the main phrases for each row in the 'info_text_en' column
houses_r['main_phrases'] = houses_r['info_text_en'].apply(extract_main_phrases)
houses_r['hotwords'] = houses_r['info_text_en'].apply(get_hotwords)
```


```{python}
# Create a list to store vectors
vector_list = []

# Iterate over each vector in the 'hotwords' column of the 'houses_r' dataframe
for vec in houses_r['hotwords']:
    # Append the vector to the 'vector_list'
    vector_list.append(vec)

# Create a list of all the words in the 'hotwords' vectors without duplicates
word_list = list(set([word for vec in vector_list for word in vec]))

# Create a new dataframe with a single column called 'word' containing all the words
hotwords = pd.DataFrame({'word': word_list})

# Add a column to the dataframe to count duplicates of each word and drop any duplicates
hotwords['duplicates_count'] = hotwords.duplicated(['word']).groupby(hotwords['word']).transform('sum')+1
hotwords = hotwords.drop_duplicates()
```


```{r}
ggplot(houses_cleaned, mapping = aes(x = built_up_area)) +
 geom_boxplot()
```

```{r}
# EDA
summary(houses_cleaned)
```
 
```{r}
 ggpairs(houses_cleaned[, c(1, 6, 7, 10, 11)], progress = FALSE)
```

```{r}

houses_BoxCox <- recipe(houses_train, price ~ .) %>%
   step_rm(lon, lat, lon_rotated, lat_rotated) %>%
   step_BoxCox(all_numeric_predictors()) %>%
   prep() %>%
   bake(new_data = NULL)

 houses_log <- recipe(houses_train, price ~ .) %>%
   step_rm(lon, lat, lon_rotated, lat_rotated) %>%
   step_log(all_numeric_predictors()) %>%
   prep() %>%
   bake(new_data = NULL)

 houses_norm <- recipe(houses_train, price ~ .) %>%
   step_rm(lon, lat, lon_rotated, lat_rotated) %>%
   step_normalize(all_numeric_predictors()) %>%
   prep() %>%
   bake(new_data = NULL)

# Plot distribution of built_up_area before and after transformations
p1 <- ggplot(data = houses_cleaned, aes(x = built_up_area)) +
  geom_density() +
  ggtitle("price before transformations")

p2 <- ggplot(data = houses_BoxCox, aes(x = built_up_area)) +
  geom_density() +
  ggtitle("price after Box-Cox transform")

p3 <- ggplot(data = houses_norm, aes(x = built_up_area)) +
  geom_density() +
  ggtitle("price after normalization")

p4 <- ggplot(data = houses_log, aes(x = built_up_area)) +
  geom_density() +
  ggtitle("price after log transform")

 (p1 + p2) / (p3 + p4)
```



```{r}
split dataframes to train(80)/test(20)

houses_cleaned <- droplevels(houses_cleaned)
set.seed(345)
houses_train_split <- initial_split(houses_cleaned, prop = 0.8, strata = price)

houses_train <- training(houses_train_split)
houses_test <- testing(houses_train_split)

set.seed(567)
houses_train_boots <- bootstraps(houses_train, times = 25)
```

```{r}
# recipes
houses_xgboost_recipe <- recipe(houses_train, price ~ .) %>%
  step_rm(lat, lon, municipality) %>%
  step_naomit(all_nominal_predictors()) %>%
  step_BoxCox(usable_area, built_up_area, land_area) %>%
  step_other(all_nominal_predictors(), threshold = 0.01) %>%
  step_dummy(all_nominal_predictors())
```


```{r}
# models
xgb_model <-
  boost_tree(
    trees = 1000, loss_reduction = tune(),
    tree_depth = tune(), min_n = tune(),
    mtry = tune(), sample_size = tune(),
    learn_rate = tune()
  ) %>%
  set_mode("regression") %>%
  set_engine("xgboost", nthread = 6) # use 6 cores for multithreading
```

```{r}
# workflows
houses_workflow <- workflow() %>%
  add_recipe(houses_xgboost_recipe) %>%
  add_model(xgb_model)
```

```{r}
# parameters
houses_xgboost_params <- parameters(
  trees(),
  tree_depth(), min_n(),
  finalize(mtry(), houses_train)
)
```

```{r}
# optimization
set.seed(789)
houses_xgboost_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), houses_train),
  learn_rate(),
  size = 30
)
```

```{r}
# results
houses_xgboost_res <- tune_grid(
  houses_workflow,
  resamples = houses_train_boots,
  grid = houses_xgboost_grid,
  control = control_grid(save_pred = TRUE)
)
```

```{r}
# best models
houses_best_model <- select_best(houses_xgboost_res, "rmse")
```

```{r}
# finalize models
houses_final_model <- finalize_model(xgb_model, houses_best_model)
houses_workflow <- houses_workflow %>% update_model(houses_final_model)
houses_xgb_fit <- fit(houses_workflow, data = houses_train)
```

```{r}
houses_xgb_fit %>%
  fit(data = houses_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

```{r}
houses_final_res <- last_fit(houses_xgb_fit, houses_train_split)
collect_metrics(houses_final_res)
# rsq is small, need to look for more predictors
```

```{r}
# evaluate
houses_pred <-
  predict(houses_xgb_fit, houses_test) %>%
  bind_cols(houses_test) %>%
  mutate(
    prediction = 10^.pred,
    real_price = 10^price
  )

houses_plot1 <-
  houses_pred %>%
  ggplot(aes(x = .pred, y = price)) +
  geom_point() +
  geom_abline(intercept = 0, col = "red")


houses_plot2 <-
  houses_pred %>%
  select(.pred, price) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, volor = key, fill = key)) +
  geom_density(alpha = .2) +
  labs(x = "", y = "")

houses_plot1 / houses_plot2
```
