---
title: "Real estate advertisements data EDA"
author: "Arnold Kakaš"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: flatly
    highlight: tango
    code_folding: hide
    toc: true
    toc_float: 
        collapsed: false
        smooth_scroll: true
    toc_depth: 4
---

<style>
table {
  white-space: nowrap;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this post I will go through <strong> Exploratory Data Analysis (EDA) </strong> of real estate advertisements data.
It is not a typical EDA, since there are already some variables excluded based on previous experience. I will examine only the variables that were proven to have reasonable impact on price prediction.
<br>
Data is scraped from [Nehnutelnosti](https://www.nehnutelnosti.sk/) website that specializes in real estate listings and services. I went through the scraping process and initial data cleaning in more details in my earlier [blog post]().
<br>

## EDA
<br>
<strong>Exploratory data analysis</strong> is an integral part of data analytics/data science.
<br>
<center>
![EDA in data science project. Source: https://commons.wikimedia.org/wiki/File:Data_visualization_process_v1.png](data/EDA.PNG)
</center>
<br>
The purpose of EDA is to summarize main characteristics of dataset, discovering patterns and relationships between the variables and identify trends. It should lead us to understanding of the data and identification of critical variables in respect to our goals.
As shown in the image, it is an iterative process. Based on your findings you can either continue with modeling/hypothesis testing and reporting or going back to data cleaning/data processing.
<br>
EDA usually starts with reading the data and checking couple of rows to get the "feeling" of data along with checking of data structure, size of the sample, data types, missing values etc.. Afterwards we continue with more detailed analysis that helps us understand the relationships and identify outliers and important variables. In EDA we use different techniques and tools. In general they can be divided into several groups:

* Summary (univariate) statistics - min, max, mean, median, quartiles, IQR, standard deviation, counts, frequency etc.
* Data visualization - histogram, boxplot, pareto chart, scatter plots, correlation matrix, line charts (for time series), heatmaps etc.
* Bi-/multi- variate statistics - t-test, chi-square test, ANOVA, Kruskal-Wallis test etc.

Lastly we create conclusion based on the findings and either continue in our data journey or go back to additional data cleaning.
<br>
Even though I prefer doing EDA manually, there are several R packages for automated EDA. I find them useful for the initial data examination and identification of e.g. data types, variables with large portion of NAs and other high level characteristics. To mention some of these packages:

* DataExplorer
* ExPanDaR
* dataMaid
* dlookr

## Data cleaning

Loading of libraries. For this part I like to use [packman](https://cran.r-project.org/web/packages/pacman/index.html) package
```{r load_libs, message=FALSE, warning=FALSE}
# load libs
pacman::p_load(bslib, # RMD theme
               Hmisc, # rcorr
               car, #leveneTest()
               rio,
               tidyverse,
               gridExtra, #plots alignment
               corrplot, # corrplot
               ggpubr,
               ggQC, # pareto chart
               rstatix, # welch test
               plotly, # interactive charts
               scales, # scales
               DT, # interactive tables
               GGally,
               patchwork,
               janitor, # clean_names
               sf) # second imported dataframe contains multipolygons
```
<br>
Next step is to load the analysed data. Usually for imports I use [rio](https://cran.r-project.org/web/packages/rio/index.html) package.
<br>
As you can see, data is stored in RDS format. I prefer this format due to several reasons: 
* size of the file is lower compared to csv or xlsx, which are usually the ones that we as data analysts are provided with.
* RDS format preserves the entire R object structure, including data frames, matrices, lists, functions, and metadata.Therefore you can save objects like dataframes with factors, trained models, etc. 
* RDS is optimized for reading and writing by the R. Therefore importing or exporting data is very quick.
<br>
Before the EDA, I need to clean up the scraped data.
<br>
I import two datasets. First one is advertisements.RDS file which contains links and basic information from the initial [webpage](https://www.nehnutelnosti.sk/byty/predaj/). I split one of the variables to obtain type of real estate.
This dataframe is joined to second one, advertisements_complete_geocoded.RDS, that contains more detailed information of each of advertisements (these informations are visible after opening the links in first dataframe) and is already geocoded with [tidygeocoder](https://jessecambon.github.io/tidygeocoder/) package using OSM Nominatim API.
<br>
Afterwards follows the <strong> data cleaning </strong> and <strong> data wrangling </strong> using different methods:

* replacing
* filtering
* imputing missing values
* converting to correct data type/creating factors
* removing redundant variables (based on further data analysis, e.g. due to high percentage of missing values or no correlation with price)
* renaming of variables
* grouping
* using geospatial joins
* removing outliers
<br>
This process is part of <strong> feature engineering </strong> for subsequent prediction model creation.
```{r data_cleaning, echo=TRUE, message=FALSE, warning=FALSE}
# load data for EDA
advertisements <- import("data/advertisements.RDS")
advertisements <- advertisements %>% separate(type_of_real_estate, c("type", "area"), sep = " • ", remove = TRUE) %>% select(link, type)


scraped_cleaned <- import("data/advertisements_complete_geocoded.RDS") %>% 
  left_join(advertisements, by = "link", multiple = "first", keep = FALSE) %>% 
  clean_names() %>%
  mutate(pocet_izieb_miestnosti = as.numeric(pocet_izieb_miestnosti),
         uzit_plocha = str_replace(str_replace(uzit_plocha, ",", "."), " m2", ""),
         energie = str_replace(str_replace(energie, ",", "."), " €/mesiac", ""),
         provizia_zahrnuta_v_cene = str_replace_na(provizia_zahrnuta_v_cene, "Nie"),
         rooms = case_when(type == "1 izbový byt" ~ 1,
                           type == "2 izbový byt" ~ 2,
                           type == "3 izbový byt" ~ 3,
                           type == "4 izbový byt" ~ 4,
                           type == "5 a viac izbový byt" ~ 5,
                           type == "Garsónka" ~ 1,
                           type == "Dvojgarsónka" ~ 2, 
                           .default = NA),
         pocet_izieb_miestnosti = coalesce(pocet_izieb_miestnosti, rooms, pocet_izieb_miestnosti)) %>% 
  mutate_at(c('index_of_living',
              'uzit_plocha',
              'energie',
              'pocet_nadzemnych_podlazi', 
              'podlazie', 
              'pocet_izieb_miestnosti', 
              'rok_vystavby', 
              'rok_poslednej_rekonstrukcie', 
              'pocet_balkonov', 
              'pocet_lodzii'), as.numeric) %>% 
  select(-link, -info_text, -address) %>% 
  filter(pocet_izieb_miestnosti < 10 & !is.na(pocet_izieb_miestnosti)) %>% 
  mutate(
         type = coalesce(type, case_when(pocet_izieb_miestnosti == 1 ~ "1 izbový byt",
                           pocet_izieb_miestnosti == 2 ~ "2 izbový byt",
                           pocet_izieb_miestnosti == 3 ~ "3 izbový byt",
                           pocet_izieb_miestnosti == 4 ~ "4 izbový byt",
                           pocet_izieb_miestnosti >= 5 ~ "5 a viac izbový byt"))) %>% 
  select(-rooms) %>% 
  filter(!(type %in% c("Apartmán", "Mezonet", "Iný byt", "Loft"))) %>% 
  mutate(rooms = case_when(type == "1 izbový byt" ~ 1,
                           type == "2 izbový byt" ~ 2,
                           type == "3 izbový byt" ~ 3,
                           type == "4 izbový byt" ~ 4,
                           type == "5 a viac izbový byt" ~ 5,
                           type == "Garsónka" ~ 1,
                           type == "Dvojgarsónka" ~ 2, 
                           .default = NA)) %>% 
  select(-c(rok_vystavby,
            rok_poslednej_rekonstrukcie,
            typ_konstrukcie,
            pocet_balkonov,
            pocet_lodzii,
            orientacia,
            energie,
            pivnica,
            vytah,
            pocet_izieb_miestnosti,
            pocet_nadzemnych_podlazi,
            podlazie)) %>% 
  dplyr::rename(index = index_of_living) %>% 
  dplyr::rename(condition = stav) %>% 
  dplyr::rename(area = uzit_plocha) %>% 
  dplyr::rename(provision = provizia_zahrnuta_v_cene) %>%
  dplyr::rename(certificate = energeticky_certifikat)

scraped_cleaned_wo_geometry <- scraped_cleaned
st_geometry(scraped_cleaned_wo_geometry) <- NULL

number_of_ads <- scraped_cleaned_wo_geometry %>% group_by(name_nsi) %>% tally() %>% arrange(n)

index_district <- scraped_cleaned_wo_geometry %>% 
  filter(!is.na(price)) %>%
  group_by(name_nsi) %>% 
  mutate(index_mean_name_nsi = mean(index, na.rm = TRUE)) %>%
  ungroup() %>%
  select(name_nsi, district, index_mean_name_nsi) %>% 
  distinct() %>% 
  mutate(index_mean_name_nsi = if_else(index_mean_name_nsi > 0, index_mean_name_nsi, NA)) %>% 
  group_by(district) %>% 
  summarize(index_mean_district = mean(index_mean_name_nsi, na.rm = TRUE))

scraped_cleaned <- scraped_cleaned %>% 
  left_join(number_of_ads, by = "name_nsi", keep = FALSE) %>% 
  filter(n > 4) %>% 
  left_join(index_district, by = "district", multiple = "first", keep = FALSE) %>% 
  st_centroid()

districts <- import("data/geospatial_data/districts.RDS")  

scraped_cleaned <- districts %>% 
  st_join(scraped_cleaned, join = st_contains) %>% 
  select(-municipality, -district) %>% 
  dplyr::rename(district = NAME) %>% 
  mutate(district = as_factor(district),
    condition = as_factor(condition),
    type = as_factor(type),
    provision = as_factor(provision),
    certificate = as_factor(certificate),
    name_nsi = as_factor(name_nsi)
  ) %>% 
  select(-n, -address1, -address2) %>% 
  filter(!is.na(price)) %>% 
  mutate_if(is.numeric,
            round,
            digits = 1)


scraped_cleaned_no_outliers <- scraped_cleaned %>% 
  group_by(name_nsi) %>% # group by municipality
  mutate(index = mean(as.numeric(index),na.rm = TRUE), # index of municipality
         index = if_else(index > 0, index, NA), # replace 0 index with NA
         IQR = IQR(price), 
         median_price = median(price),
         lower = median_price - 1.5*IQR, 
         upper = median_price + 1.5*IQR
         ) %>% 
  ungroup() %>% 
  filter(price >= lower & price <= upper) %>% # filter out outliers
  select(-lower, -upper, -median_price, -IQR)# remove variables

st_geometry(scraped_cleaned_no_outliers) <- NULL

# scraped_cleaned_no_outliers <- scraped_cleaned_no_outliers %>% 
#   group_by(district) %>% 
#   mutate(avg_price_district = mean(price, na.rm = TRUE),
#          )
```

```{r remove_objects, message=FALSE, warning=FALSE, include=FALSE}
# Remove all dataframes except of scraped_cleaned_no_outliers
rm(list = setdiff(ls(), "scraped_cleaned_no_outliers"))
gc()
```

## EDA of advertisements dataset {.tabset .tabset-pills}
<br>
In this section I will go through EDA process using the methods I already mentioned.
<br>
Before I start, there is one more thing I need to discuss.
There are several assumption of t-test/ANOVA:

* data is normally distributed
* distributions have the same variance
* data are independent

In reality, the first two assumption are often ignored if you are dealing with large samples. There are also variations/arguments of R formulas (e.g. var.equal = FALSE in `oneway.test()`)
<br>
### Overview
<br>
Let's check how the data look like

```{r show_head, echo=TRUE}
head(scraped_cleaned_no_outliers) %>% 
  mutate_if(is.numeric,
            round,
            digits = 1) %>% 
  datatable(
            rownames = FALSE,
            options = list(scrollX = TRUE,
                           autoWidth = TRUE,
                           columnDefs = list(list(width = '125px', targets = c(0, 1, 8))
                                             )
                           )
            )#%>% 
  # tab_options(table.font.size = 10,
  #             table.width = 1100,
  #             table.layout = "auto")
```

<br>
I like to start the EDA with base summary function.
<br>
```{r summary, echo=TRUE, message=FALSE, warning=FALSE}
 summary(scraped_cleaned_no_outliers) %>% 
  as.data.frame()  %>%
  select(-Var1) %>% 
  pivot_wider(names_from = Var2, values_from = Freq) %>% 
  unnest(everything()) %>% 
  dplyr::rename('district mean index' = 11) %>% 
  dplyr::rename(municipality = 2) %>% 
  datatable(rownames = FALSE,
            options = list(scrollX = TRUE,
                           autoWidth = TRUE,
                           columnDefs = list(list(width = '125px', targets = "_all")
                                             )
                           ))
```
<br>
Generally, there is a lot of missing data. This issue will be handled by imputation directly in model steps set up.

* Most of the advertisements are from western part of Slovakia, we can see clear dominance of Bratislava city districts. Looking at the top 5 municipalities, only Banská Bystrica is located outside of the western part of country.
* For municipalities I will not do EDA since there are more than 200 unique values. Only to put this variable into perspective, there is 2890 municipalities (considering Bratislava and Košice as single entities without split to city districts). Only around 150 of them have more than 5000 inhabitants.
More interesting information on cities and towns in Slovakia can be found on [this](https://www.sodbtn.sk/) webpage.
* Price has quite big range from 1000 to almost 700k, but the middle 50 % lies within 75k range from 105,000 to 180,000. We will check the distribution later in histogram.
* Index can have value from 0 to 10 and it is calculated by slovak startup [City Performer](https://cityperformer.com/). It is made up of six categories: environment, quality of housing, safety, transport, services and relaxation. That being said, it is not available for all places.
Middle 50 % are within 7.1 to 8.1. Mean index is 7.3 points.
* There are 6 possible conditions + NA. The most common is complete reconstruction followed by partial reconstruction and new construction.
* Area is one of the most important attributes impacting the price. As we can see, we it ranges from 1 squared meter to 59,000, both are obviously incorrect values. Similarly to price, we will look at distribution in histogram.
* Provision tells us whether the provision of real estate agency is included. Mostly it is not (only part of the apartments are sold via real estate agency).
* Information on energy certificate is mostly missing or the property does not have one. But in cases of high grade certificates we can expect positive impact on price.
* Type and rooms are two variables showing very similar characteristic. Most common are apartments with 3 and 2 rooms. 
* Mean index value of district is used as additional feature to evaluate the location quality for places where local index is not available. Nevertheless, there are places where even this aggregated index could not be calculated, nor by grouping or spatial lag.

### Price
Price is the most important variable in this dataset and it is the one I will try to predict with ML model.

```{r price_histo, echo=TRUE, message=FALSE, warning=FALSE}
price1 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = price)) +
        geom_histogram(fill = "steelblue", binwidth = 10000) +
        scale_x_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  theme_minimal()
        
price2 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = log10(price))) +
        geom_histogram(fill = "steelblue") +
  theme_minimal()

grid.arrange(price1, price2, ncol = 1)
```
<br>
```{r price_boxplot, echo=TRUE, message=FALSE, warning=FALSE}
price3 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(y = price)) +
  geom_boxplot(col = 'steelblue') + 
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  theme_minimal()

price4 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(y = log10(price))) +
  geom_boxplot(col = 'steelblue') + 
  theme_minimal()

grid.arrange(price3, price4, ncol = 2)
```

```{r qq_price, echo=TRUE}
qqnorm(scraped_cleaned_no_outliers$price, pch = 1, frame = FALSE)
qqline(scraped_cleaned_no_outliers$price, col = "steelblue", lwd = 2)
```

```{r qq_price_log, echo=TRUE}
qqnorm(log10(scraped_cleaned_no_outliers$price), pch = 1, frame = FALSE)
qqline(log10(scraped_cleaned_no_outliers$price), col = "steelblue", lwd = 2)
```
<br>
As we can see, the distribution is skewed to the right, but this was expected since there are some expensive apartments in limited numbers.
Log transformation help with this issue as confirmed by all plots in this section. There is only one case that should be removed with log value of 3.

### District
Regional differences in Slovakia have west east gradient in many socio-cultural aspects. Therefore, it is reasonable to expect the same pattern also in the prices of apartments.
```{r top_10_districts, echo=TRUE, message=FALSE, warning=FALSE}
top_bottom_10_districts <- scraped_cleaned_no_outliers %>% 
  group_by(district) %>% 
  summarize(price = mean(price, na.rm = TRUE),
            adverts_count = n()) %>% 
   filter(!between(dense_rank(price), 10 + 1, n() - 10)) %>% 
  arrange(desc(price))
```
<br>
```{r top_10_districts_chart, echo=TRUE, message=FALSE, warning=FALSE}
top_10_districts_chart <- top_bottom_10_districts %>% 
  ggplot(aes(x = reorder(district, -price), y = price, text = paste0("Price: ",
                                                                     format(round(price, 0),big.mark=" ",))
  )) + 
  geom_bar(stat='summary', fun.y = "mean", fill = 'steelblue') +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45)) + 
  scale_y_continuous(breaks = seq(0, 350000, by = 50000), labels = comma) + 
  geom_text(aes(x = reorder(district, -price) ,y = price + 25000, label = adverts_count), size = 3) +
  geom_hline(yintercept = 135900, linetype = "dashed", color = "red") + #dashed line is median price 
  labs(x = "district")

ggplotly(top_10_districts_chart, tooltip = "text")
```
<br>
It is confirmed in the chart. We can see that most expensive apartments are from western part of Slovakia with exceptions of Košice I and Poprad which are regional centers in the east. On the other hand, all of the bottom 10 districts are located in eastern part. There I need to note, that most of these districts have very low number of observations (advertisements). This can tell us two things:

* The low prices can be due to chance
* The real estate market in this part of Slovakia is less developed.

### Index
As mentioned earlier, Index gives us aggregated value that aims to evaluate the quality of place where the apartment listed in the advertisement is located.
```{r index_isna, echo=TRUE, message=FALSE, warning=FALSE}
sum(is.na(scraped_cleaned_no_outliers$index))
sum(is.na(scraped_cleaned_no_outliers$index))/nrow(scraped_cleaned_no_outliers)
```
<br>
There is 1828 (16.1% of total) advertisements where index was not available. 
<br>
```{r index_histo, echo=TRUE, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  select(index) %>% 
  drop_na() %>% 
  ggplot(aes(x = index)) +
        geom_histogram(fill = "steelblue") +
  theme_minimal()
```
<br>
Distribution of the values is concentrated around higher values (7.5+) with significant negative skewness.
<br>
```{r index_price, echo=TRUE, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  select(price, index) %>% 
  drop_na() %>% 
  ggplot(aes(x = index, y = price)) +
  geom_point(col = 'steelblue') + 
  geom_smooth(method = "lm", se = FALSE, color = "red", aes(group = 1)) +
  stat_regline_equation(label.x = 1.5, label.y = 650000) + # for regression equation 
  stat_cor(aes(method = "pearson", label = after_stat(rr.label)), label.x = 1.5, label.y = 600000) + # for R^2 
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma)
```
<br>
Looking at the linear regression model, index explains roughly 10% of variance in price. I will check the correlation and p-values later in the EDA.

### Condition
```{r cond_pareto, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  group_by(condition) %>% 
  summarize(count = n()) %>% 
  ggplot(aes(x = condition, y = count)) +
  ggQC::stat_pareto(point.color = "red",
                   point.size = 2,
                   line.color = "black",
                   bars.fill = "steelblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  scale_x_discrete(labels = wrap_format(10)) +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```
<br>
Pareto diagram shows that the decline in group size is very moderate and regular. Bigger drop is only in the group In construction ("Vo výstavbe") and Developer project ("Developerský projekt"). Most common are Complete reconstruction ("Kompletná rekonštrukcia") and Partial reconstruction ("Čiastočná rekonštrukcia"), followed by New construction ("Novostavba") and Original state ("Pôvodný stav"). 
<br>
```{r condition_boxplot, echo=TRUE, message=FALSE, warning=FALSE}
stat_box_data_prov <- function(y, upper_limit = max(scraped_cleaned_no_outliers$price) * 1.05) {
  return(
    data.frame(
      y = 0.95 * upper_limit,
      label = paste('n =', length(y), '\n',
                    'x\u0305', ' =', round(mean(y), 0), '\n')
    )
  )
}

cond1 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = factor(condition), y = price)) +
  geom_boxplot(col = 'steelblue') + 
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  scale_x_discrete(labels = wrap_format(10)) +
  stat_summary(
    fun.data = stat_box_data_prov, 
    geom = "text", 
    size = 3
  ) + 
  theme_minimal() +
  labs(x = "type")

ggplotly(cond1)
```

There are obvious differences in price among condition groups. Highest values are in three categories: In construction, Developer project and New construction. Not a big surprise that new and not yet finished apartments are the priciest.
On the other hand, the cheapest are Original state and Partial reconstruction. Somewhere in the middle sits Complete reconstruction.
All Condition groups have outliers in higher price ranges. Distribution is positively skewed.
<br>
```{r condition_sd, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  group_by(condition) %>% 
  get_summary_stats(price, type = "mean_sd") %>%
  datatable(
    rownames = FALSE,
    )
```
<br>
Let's check the differences in means of these groups. Even though the normality assumption is violated, we have reasonably large sample sizes and the ANOVA can give us useful insight. I ran this test with log transformed prices and results were the same.
<br>
```{r condition_price_log_df, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
cond_anova_df <- scraped_cleaned_no_outliers %>% 
  select(price, condition) %>% 
  mutate(price = log10(price))
```

```{r leveneTest_condition}
leveneTest(scraped_cleaned_no_outliers$price, scraped_cleaned_no_outliers$condition) 
```
The null hypothesis of equal variances is rejected (p < .001), so we should continue with a t-test that assumes unequal variances.

```{r aov_condition, message=FALSE, warning=FALSE}
cond_aov <- aov(price ~ condition, data = scraped_cleaned_no_outliers)
summary(cond_aov)
```
<br>
Since the p-value is lower then 0.001 we can conclude that there is significant difference between groups. Now we need to make a post hoc analysis to check which groups are different.
<br>
```{r condition_pwc, message=FALSE, warning=FALSE}
cond_res.aov <- scraped_cleaned_no_outliers %>%
  filter(!is.na(condition)) %>%
  anova_test(price ~ condition)

cond_pwc <- scraped_cleaned_no_outliers %>%
  filter(!is.na(condition)) %>%
  pairwise_t_test(price ~ condition, #var.equal is FALSE by default 
    p.adjust.method = "bonferroni"
  ) %>%
  mutate(
    p = round(p, 5),
    p.adj = round(p.adj, 5)
  )

cond_pwc %>%
  datatable(
    rownames = FALSE,
    options = list(
      scrollX = TRUE,
      autoWidth = TRUE,
      columnDefs = list(list(width = "125px", targets = c(1, 2)))
    )
  )
```
```{r}
cond_pwc <- cond_pwc %>% add_xy_position(x = "condition")

ggboxplot(scraped_cleaned_no_outliers %>% 
            filter(!is.na(condition)), 
          x = "condition", 
          y = "price", col = 'steelblue') +
  stat_pvalue_manual(cond_pwc, hide.ns = TRUE, label = "p.adj.signif") +
  labs(
    subtitle = get_test_label(cond_res.aov, detailed = TRUE),
    caption = get_pwc_label(cond_pwc)
    ) +
   scale_x_discrete(labels = wrap_format(10)) +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```
<br>
From the table and chart it can be seen that there are significant differences between groups which are highlighted with “*“.

### Area
I expect the area to be one of the most important features impacting the price. The greater is the living area, the higher is the price. Obviously area is closely tied with e.g. type of apartments. More rooms usually means more space. 
<br>
```{r area_histo, echo=TRUE, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  filter(area < 200 & area > 10) %>% 
  select(area) %>% 
  drop_na() %>% 
  ggplot(aes(x = area)) +
        geom_histogram(fill = "steelblue") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```
<br>
Distribution of area reminds the normal distribution the most out of all predictors we have in dataset. These is a skeweness to the right due to limited number of apartments with large living area. As you probably noticed, I filtered out all advertisements with area lower or equal to 10 and higher or equal to 200. I suspect that these are mistakes/intentionally wrong records.
<br>
```{r qq_area, echo=TRUE}
qq_area <- scraped_cleaned_no_outliers %>% 
  select(area) %>% 
  filter(area > 10 & area < 200)

qqnorm(qq_area$area, pch = 1, frame = FALSE)
qqline(qq_area$area, col = "steelblue", lwd = 2)
```
<br>
QQ plot is another way how to check the normality of distribution. Again, the skewness to the right is clearly visible.
<br>
```{r area_price, echo=TRUE, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  filter(area < 200 & area > 10) %>% 
  ggplot(aes(x = area, y = price)) +
  geom_point(col = 'steelblue') + 
  geom_smooth(method = "lm", se = FALSE, color = "red", aes(group = 1)) +
  stat_regline_equation(label.x = 3, label.y = 650000) + # for regression equation 
  stat_cor(aes(method = "pearson", label = after_stat(rr.label)), label.x = 3, label.y = 600000) + # for R^2 
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma)
```
<br>
Area explains 24% of variance in price. This is much more compared to other numeric variables.
On the other hand, it seems like the correlation is probably quite low. Price varies a lot in all area sizes, but there is visible trend of increasing price variance with increasing area.
I read it as there is more than only area what comes into price determination. Two apartments with the same area can have very different price e.g. due to location, equipment of the apartment, age, condition etc..

### Provision
```{r provision_boxplot, echo=TRUE, message=FALSE, warning=FALSE}
stat_box_data_prov <- function(y, upper_limit = max(scraped_cleaned_no_outliers$price) * 1.05) {
  return(
    data.frame(
      y = 0.95 * upper_limit,
      label = paste('n =', length(y), '\n',
                    'x\u0305', ' =', round(mean(y), 0), '\n')
    )
  )
}

prov1 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = factor(provision), y = price)) +
  geom_boxplot(col = 'steelblue') + 
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  stat_summary(
    fun.data = stat_box_data_prov, 
    geom = "text", 
    size = 3
  ) + 
  theme_minimal() +
  labs(x = "provision")

ggplotly(prov1)
```
<br>
Interestingly enough it seems like the apartments with provision included in price ("Áno") have lower price then those without it ("Nie"). This may be due to fact that, real estate agencies know the market better than private advertisers and keep the price more reasonable. Also it can be due to mix of types in both baskets. Nevertheless, both groups have similar values of all statistical measures displayed in box-plots.
<br>

```{r leveneTest_provision}
leveneTest(scraped_cleaned_no_outliers$price, scraped_cleaned_no_outliers$provision) 
```
The null hypothesis of equal variances is rejected (p ~ .001), so we should continue with a t-test that assumes unequal variances.

To check whether the difference of means is really significant, we can do a one way ANOVA.
<br>
```{r onewaytest_provision}
oneway.test(price ~ provision, data = scraped_cleaned_no_outliers)
```
<br>
We can conclude that there are significant differences between the groups (p < 0.001).
<br>
Normality check:
<br>
```{r}
plot(res.aov, 2)
```
<br>
We can clearly see that data is not normally distributed. First assumption is violated. 
<br>
Homogenity of variance: 
<br>
```{r}
plot(res.aov, 1)
```
<br>
Outliers are discovered in multiple points, which can have a significant impact on normality and homogeneity of variance and it can be beneficial to remove these outliers.
<br>
Independence:
Data are independent.
<br>
On the first glance, we failed to reject the null hypothesis since the assumptions were violated. Nevertheless, since we have large dataframe and the difference is visible in all descriptive points (quantiles, mean, etc.), I feel confident to say that there is practical significance in this variable.

### Certificate
<br>
The energy certificate of a building (ECB) is a legal document that evaluates buildings in terms of energy efficiency and carbon dioxide emissions, resulting in their classification in energy scales from the most efficient class A to the wasteful class G.
<br>
The energy certificate evaluates buildings in terms of 4 points of energy consumption:

* thermal protection of buildings
* energy requirement for heating and preparation of hot water
* energy need for air conditioning, air conditioning
* energy need for built-in wiring, lighting

Based on these points of consumption, the ECB recalculates the required amount of energy in kWh / m² per year for the entire building
<br>
```{r cert_pareto, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  group_by(certificate) %>% 
  summarize(count = n()) %>% 
  ggplot(aes(x = certificate, y = count)) +
  ggQC::stat_pareto(point.color = "red",
                   point.size = 2,
                   line.color = "black",
                   bars.fill = "steelblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  scale_x_discrete(labels = wrap_format(10)) +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```
<br>
67% of the advertised apartments do not have listed the ECB. Almost 20% does not have any certificate. The least frequent certificates are E and F with only 1 observation for each. 
<br>
```{r cert_boxplot, echo=TRUE, message=FALSE, warning=FALSE}
stat_box_data <- function(y, upper_limit = max(scraped_cleaned_no_outliers$price) * 1.05) {
  return(
    data.frame(
      y = 0.95 * upper_limit,
      label = paste('n =', length(y), '\n',
                    'x\u0305', ' =', round(mean(y), 0), '\n')
    )
  )
}

cert1 <- scraped_cleaned_no_outliers %>%
  filter(!certificate %in% c("E", "F")) %>% 
  ggplot(aes(x = factor(certificate, levels = c("A",
                                         "B",
                                         "C",
                                         "D",
                                         "G",
                                         "nemá")), y = price)) +
  geom_boxplot(col = 'steelblue') + 
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  stat_summary(
    fun.data = stat_box_data, 
    geom = "text", 
    size = 3
  ) + 
  theme_minimal() +
  labs(x = "type")

ggplotly(cert1)
```
<br>
The box-plot chart nicely illustrates the relationship between certificate level and price. With decreasing level of certificate, price drops as well. That being said, there are other factors as well. I suspect that A levels will be probably represented by relatively new apartments which have higher prices. 
Another topic is group with no certificate ("nemá") and without information on certificate (NA values). It could be tempting to merge them together, however there is quite significant difference in descriptive statistics of these two groups. I will use imputation step in model recipe to replace missing values.
<br>
```{r cert_sd, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  filter(!certificate %in% c("E", "F")) %>% 
  group_by(certificate) %>% 
  get_summary_stats(price, type = "mean_sd") %>%
  datatable(
    rownames = FALSE,
    )
```
<br>
Normality assumption for ANOVA is violated also in this case but in smaller extend as in Condition groups. Again, I ran this test with log transformed prices and results were the same. I excluded "E", "F" (only 1 observation) and NA groups. Also in this case I ran the analysis with log transformed price with similar results.
<br>
```{r cert_price_log_df, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
cert_anova_df <- scraped_cleaned_no_outliers %>% 
  select(price, certificate) %>% 
  mutate(price = log10(price))
```
<br>
```{r cert_AOV, message=FALSE, warning=FALSE}
cert_AOV <- aov(price ~ certificate, data = scraped_cleaned_no_outliers)
summary(cert_AOV)
```
<br>
The F-value is much bigger then 1 and p-value is lower then 0.05, we can conclude that there is significant difference between groups. Let's continue with a post hoc analysis to check which groups are different.
<br>
```{r cert_pwc, message=FALSE, warning=FALSE}
cert_res.aov <- scraped_cleaned_no_outliers %>%
  filter(!is.na(certificate)) %>%
  filter(!certificate %in% c("E", "F")) %>% 
  anova_test(price ~ certificate)

cert_pwc <- scraped_cleaned_no_outliers %>%
  filter(!certificate %in% c("E", "F")) %>% 
  filter(!is.na(certificate)) %>%
  pairwise_t_test(price ~ certificate,
    p.adjust.method = "bonferroni"
  ) %>%
  mutate(
    p = round(p, 5),
    p.adj = round(p.adj, 5)
  )

cert_pwc %>%
  datatable(
    rownames = FALSE,
    options = list(
      scrollX = TRUE,
      autoWidth = TRUE,
      columnDefs = list(list(width = "125px", targets = c(1, 2)))
    )
  )
```
<br>
```{r}
cert_pwc <- cert_pwc %>% add_xy_position(x = "certificate")

ggboxplot(scraped_cleaned_no_outliers %>% 
            filter(!is.na(certificate)) %>% 
            filter(!certificate %in% c("E", "F")), 
          x = "certificate", 
          y = "price", col = 'steelblue') +
  stat_pvalue_manual(cert_pwc, hide.ns = TRUE, label = "p.adj.signif") +
  labs(
    subtitle = get_test_label(cert_res.aov, detailed = TRUE),
    caption = get_pwc_label(cert_pwc)
    ) +
   scale_x_discrete(labels = wrap_format(10)) +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```
<br>
From the table and chart it can be seen that there are significant differences between groups which are highlighted with “*“.

### Type
<br>
There are 7 types of apartments in this dataset:

* 1 room apartment*
* 2 rooms apartment
* 3 rooms apartment
* 4 rooms apartment
* 5+ rooms apartment
* studio apartment ("Garsónka")
* one-bedroom apartment ("Dvojgarsónka")*

*The difference between 1 room apartment and one-bedroom apartment is that 1 room apartment has one living/bed room and separate kitchen and one-bedroom apartments has two separate rooms (typically bedroom and living room) and kitchen in part of one of them.
<br>
```{r type_pareto, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  group_by(type) %>% 
  summarize(count = n()) %>% 
  ggplot(aes(x = type, y = count)) +
  ggQC::stat_pareto(point.color = "red",
                   point.size = 2,
                   line.color = "black",
                   bars.fill = "steelblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  scale_x_discrete(labels = wrap_format(10)) +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```
<br>
The most common types are 3 rooms (41%) and 2 rooms (35%) apartments. Following 1 room apartments count drops by almost 2/3. 
<br>
```{r type_boxplot, echo=TRUE, message=FALSE, warning=FALSE}
stat_box_data <- function(y, upper_limit = max(scraped_cleaned_no_outliers$price) * 1.05) {
  return(
    data.frame(
      y = 0.95 * upper_limit,
      label = paste('n =', length(y), '\n',
                    'x\u0305', ' =', round(mean(y), 0), '\n')
    )
  )
}

type1 <- scraped_cleaned_no_outliers %>% 
  ggplot(aes(x = factor(type, levels = c("1 izbový byt",
                                         "2 izbový byt",
                                         "3 izbový byt",
                                         "4 izbový byt",
                                         "5 a viac izbový byt",
                                         "Garsónka",
                                         "Dvojgarsónka")),
             y = price)) +
  geom_boxplot(col = 'steelblue') + 
  scale_x_discrete(labels = wrap_format(15)) +
  stat_summary(
    fun.data = stat_box_data, 
    geom = "text", 
    size = 3
  ) + 
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma) +
  theme_minimal() +
  labs(x = "type")

ggplotly(type1)
```
<br>
There are two interesting findings from the chart above. 
First one, 2 and 3 rooms apartments have very similar prices. 3 rooms are obviously more expensive but the difference, especially when we consider price increase from 1 room to 2 rooms and from 3 rooms to 4 rooms, is very mild. 
Second finding, one-bedroom apartments seem to fit between 1 and 2 rooms apartments rather then being an alternative to 1 room apartments (I admit that number of observations is very small and this can be due to chance). 
<br>
```{r type_sd, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  filter(!type %in% c("E", "F")) %>% 
  group_by(type) %>% 
  get_summary_stats(price, type = "mean_sd") %>%
  datatable(
    rownames = FALSE,
    )
```
<br>
Yet again, normality assumption for ANOVA is violated. Log transformation of price and consecutive ANOVA returned similar results as the ones below.
<br>
```{r type_price_log_df, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
type_anova_df <- scraped_cleaned_no_outliers %>% 
  select(price, type) %>% 
  mutate(price = log10(price))
```
<br>
```{r type_AOV, message=FALSE, warning=FALSE}
type_AOV <- aov(price ~ type, data = scraped_cleaned_no_outliers)
summary(type_AOV)
```
<br>
The F-value is much bigger then 1 and p-value is lower then 0.001, we can conclude that there is significant difference between groups. Let's continue with a post hoc analysis to check which groups are different.
<br>
```{r type_pwc, message=FALSE, warning=FALSE}
type_res.aov <- scraped_cleaned_no_outliers %>%
  filter(!is.na(type)) %>%
  #filter(!type %in% c("E", "F")) %>% 
  anova_test(price ~ type)

type_pwc <- scraped_cleaned_no_outliers %>%
  #filter(!type %in% c("E", "F")) %>% 
  filter(!is.na(type)) %>%
  pairwise_t_test(price ~ type,
    p.adjust.method = "bonferroni"
  ) %>%
  mutate(
    p = round(p, 5),
    p.adj = round(p.adj, 5)
  )

type_pwc %>%
  datatable(
    rownames = FALSE,
    options = list(
      scrollX = TRUE,
      autoWidth = TRUE,
      columnDefs = list(list(width = "125px", targets = c(1, 2)))
    )
  )
```
<br>
```{r}
type_pwc <- type_pwc %>% add_xy_position(x = "type")

ggboxplot(scraped_cleaned_no_outliers %>% 
            filter(!is.na(type)),# %>% 
            #filter(!type %in% c("E", "F")), 
          x = "type", 
          y = "price", col = 'steelblue') +
  stat_pvalue_manual(type_pwc, hide.ns = TRUE, label = "p.adj.signif") +
  labs(
    subtitle = get_test_label(type_res.aov, detailed = TRUE),
    caption = get_pwc_label(type_pwc)
    ) +
   scale_x_discrete(labels = wrap_format(10)) +
  theme_minimal()
```
<br>
From the table and chart it can be seen that there are significant differences between groups which are highlighted with “*“.

### Mean District Index
As already mentioned, this variable serves as a extra feature for advertisements where index was not available. The assumption of this thought is that there is a spatial autocorrelation of features stepping into calculation of index.
<br>
```{r index_district_isna, echo=TRUE}
sum(is.na(scraped_cleaned_no_outliers$index_mean_district))
sum(is.na(scraped_cleaned_no_outliers$index_mean_district))/nrow(scraped_cleaned_no_outliers)
```
<br>
Since there is 1242 rows (11% of total) with missing value, this feature covers approx. 600 cases of missing index value.
<br>
```{r index_district_histo, echo=TRUE, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  select(index_mean_district) %>% 
  drop_na() %>% 
  ggplot(aes(x = index_mean_district)) +
        geom_histogram(fill = "steelblue") +
  theme_minimal() +
  labs(x = "district mean index")
```
<br>
Compared to index, district mean distribution resembles the normal distribution to higher extent, however is it still heavily skewed to the felt.
```{r index_districts_price, echo=TRUE, message=FALSE, warning=FALSE}
scraped_cleaned_no_outliers %>% 
  select(price, index_mean_district) %>% 
  drop_na() %>% 
  ggplot(aes(x = index_mean_district, y = price)) +
  geom_point(col = 'steelblue') + 
  geom_smooth(method = "lm", se = FALSE, color = "red", aes(group = 1)) +
  stat_regline_equation(label.x = 1.5, label.y = 650000) + # for regression equation 
  stat_cor(aes(method = "pearson", label = after_stat(rr.label)), label.x = 1.5, label.y = 600000) + # for R^2 
  theme_minimal() +
  labs(x = "district mean index") +
  scale_y_continuous(breaks = seq(0, 700000, by = 100000), labels = comma)
```
<br>
It explains only 7.6% of price variance. Again, the correlation and p-values are discussed in next part of EDA.

### Correlation matrix
<br>
```{r corr, echo=TRUE}
all_num_pred <- select_if(scraped_cleaned_no_outliers, is.numeric) %>% drop_na()

M <- cor(all_num_pred)

corrplot(M, method = 'color', order = 'AOE', type = 'upper', addCoef.col = 'black', diag = FALSE, )
```

There is a strong multicollinearity between index and mean district index. SPOILER ALERT Since the final model is an XGBoost model, I don't consider this a big issue. On the other hand, it may be beneficial in cases where there is no index value available (approx. 600 cases).
<br>
No correlation between price and area is confirmed as well.
Below are the results including p-values:
```{r corr_table, message=FALSE, warning=FALSE}
rcorr(as.matrix(all_num_pred), type = "pearson")
```

## Conclusion
<br>
EDA provided us with valuable insights, that will be considered in the prediction model:

* Price distribution is skewed to the right - there are limited offers of expensive apartments
* Geospatial distribution has west east gradient - lower prices are in the east, with exception of few regional centers.
* Most locations in our dataset has quite high level of Index of living. But in general there is a positive relation between its value and price.
* Surprisingly, Area itself does not impact the price as much as I expected. Even though $R^2$ is 0.24, the p value is high. I assume this is due to wider context of pricing. Other qualitative characteristics over perform the area size.
* There are proven differences between prices of apartments with different Conditions. It is not surprising that new apartments have the highest prices. There are more then 800 NAs, which need to be imputed.
* Also certificate has proven impact, in this case the issue with NAs is more serious as more then 7,500 advertisements of not contains this information. I will still impute this variable as I assume that there is a connection between Certificate and Condition
* Vast majority of apartments in data set have 2 and 3 rooms. The price is growing with increasing size group. Price increase from 2 rooms to 3 rooms group is quite low on average. I can see two possible reasons - demand for 2 rooms apartments (as they are still cheaper than 3 rooms) and location. If majority of 2 rooms apartments would be located in the west, they price would be on average for whole country higher compared to equal spatial distribution.